[  
  {"article": "]\nWorldBench Team\nEqual Contributions\nProject Lead\nCorresponding Author\nWorldLens : Full-Spectrum Evaluations of Driving World Models in Real World\nAbstract\nGenerative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens , a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects – Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference – jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity – standardizing how future models are judged not only by how real they look, but by how real they behave.\n[\nProject Page][URL_REMOVED]\n\\metadata[\nGitHub Repo][URL_REMOVED]\n\\metadata[\nHuggingFace Leaderboard][URL_REMOVED]\n\\metadata[\nHuggingFace Dataset][URL_REMOVED]\n[ADDRESS_REMOVED] transformed embodied AI and simulation [77, 3, 82, 86, 136]. From text-driven 4D synthesis to controllable driving environments [89, 121, 72, 139, 103, 104], modern systems can produce dash-cam–like sequences with striking visual realism. However, evaluation has not kept pace: the field lacks a standardized way to measure whether generated worlds preserve geometry, respect physics, and support reliable decision-making [53, 78, 107].\nMost widely used metrics emphasize frame quality and aesthetics [49, 41, 42, 1], but reveal little about physical causality, multi-view geometry, or functional reliability under control [124, 29, 5, 131, 120, 146, 143]. This gap, which is well documented across recent surveys [53, 107, 141, 68], has created fragmented progress and incomparable results. While structured maturity scales, e.g., SAE Levels of Driving Automation, have clarified autonomy benchmarking [73], an analogous, practice-ready protocol for evaluating driving world models has remained elusive.\nTo bridge the gap, we build WorldLens , a full-spectrum benchmark that evaluates how well a world model builds, understands, and behaves within its generated world. As shown in Figure 1, no existing model excels universally; some achieve strong texture realism but violate physics, while others preserve geometry yet fail behaviorally.\nTo reveal these world modeling trade-offs, we decompose evaluation into five complementary aspects:\n-\n•\n1Generation — measuring whether a model can synthesize visually realistic, temporally stable, and semantically consistent scenes [27, 89, 30]. Even state-of-the-art models that achieve low perceptual error (e.g., LPIPS, FVD) often suffer from view flickering or motion instability, revealing the limits of current diffusion-based architectures.\n-\n•\n2Reconstruction — probing whether generated videos can be reprojected into a coherent 4D scene using differentiable rendering [17, 50]. Models that appear sharp in 2D frequently collapse when reconstructed, producing geometric “floaters”: a gap that exposes how temporal coherence remains weakly coupled in most pipelines.\n-\n•\n3Action-Following — testing if a pre-trained action planner [38, 45] can operate safely inside the generated world. High open-loop realism does not guarantee safe closed-loop control; almost all existing world models trigger collisions or off-road drifts, underscoring that photometric realism alone cannot yield functional fidelity.\n-\n•\n4Downstream Task — evaluating whether the synthetic data support downstream perception models trained on real-world datasets [66, 96, 9]. Even visually appealing worlds may degrade detection or segmentation accuracy by –, highlighting that alignment to task distributions, not just image quality, is vital for practical usability.\n-\n•\n5Human Preference — capturing subjective scores such as world realism, physical plausibility, and behavioral safety through large-scale human annotations. Our study reveals that models with strong geometric consistency are generally rated as more “real”, confirming that perceptual fidelity is inseparable from structural coherence.\nTo bridge algorithmic metrics with human perception, we curate WorldLens-26K, a large-scale dataset of human-annotated videos covering perceptual, physical, and safety-related dimensions. Each sample contains quantitative scores and textual explanations, capturing how evaluators reason about realism, physical plausibility, and behavioral safety. By pairing human judgments with structured rationales, we aim to transform subjective evaluation into learnable supervision, enabling perception-aligned and interpretable assessment of generative world models.\nLeveraging the above, we develop WorldLens-Agent, a feedback-aligned auto-evaluator distilled from human preferences. This agent can predict perceptual and physical scores while generating natural-language explanations consistent with human reasoning. It generalizes well to unseen models and enables scalable auto-evaluation of generative worlds without repeated manual labeling.\nTogether, our benchmark, dataset, and evaluation agent form a unified ecosystem that bridges objective measurement and human interpretation. We will release the toolkit, dataset, and model to foster standardized, explainable, and human-aligned evaluation – guiding future world models not only to “look” real, but to “behave” reasonably.\n2 Related Work\nVideo Generation. Recent advances have driven rapid progress in generation [119, 68, 55, 4, 106, 136]. Text-to-image [87, 6, 85, 114] laid the foundation for high-fidelity synthesis from text, later extended to the temporal domain through text-to-video (T2V) systems [91, 8, 54, 82, 26, 90, 36, 100, 43, 99, 97, 98, 74, 132]. Building on these foundations, domain-specific methods [94, 56, 105, 62, 67, 27, 32, 116, 145] achieved impressive realism using motion-aware conditioning [7, 89, 39, 44, 108, 115, 46]. Despite strong perceptual quality, they remain largely appearance-driven: they generate visually coherent sequences but lack explicit geometry, physics, or causal control [121, 120, 5, 14, 130]. Without structured world states or dynamics, they cannot model how scenes behave or respond to actions [20, 1, 125], and metrics focused only on visual fidelity reveal little about whether a model truly understands the world it depicts.\n3D & 4D World Modeling. Recent studies move beyond frame-based generation to build world models that encode 3D, dynamics, and control-aware representations [53]. WonderWorld [135], GAIA-1/2 [37, 86], Genie-3 [3], and related efforts [133, 31, 29, 124] learn physics-grounded latent states representing geometry, occupancy, and motion for conditioned predictions [16, 5, 60, 65, 147, 129, 70, 59, 52]. DriveArena [127] and NAVSIM [20, 11] integrate perception, planning, and control into generative pipelines that support closed-loop simulation [10, 23, 123]. Yet, existing paradigms remain limited to perception metrics or qualitative results [53]. A unified standard for measuring geometry, 4D consistency, and agent behaviors is missing.\nEvaluation. Generative video evaluation has evolved from simple frame-based scores to multi-dimensional benchmarks [61, 57, 102, 63, 137]. Early metrics measure distributional similarity and perceptual alignment [35, 88, 101, 79, 49], while frameworks, e.g., VBench [41, 42], EvalCrafter [64], and T2V-CompBench [93], extend assessment to motion and temporal consistency. More recent efforts, WorldScore [25] and VideoWorld [84], move toward “world-model” evaluation using physics-inspired composite scores, yet they remain confined to 2D video settings emphasizing appearance over embodiment [47]. In driving, some existing benchmarks [23, 10, 118] evaluate agents rather than the worlds they inhabit. WorldLens introduces 4D reconstruction, action-following, and human preference alignment to jointly assess spatial fidelity, behavioral consistency, and physical realism, establishing the first benchmark that measures both the appearance and behavior.\n3 WorldLens : A Full-Spectrum Benchmark\nGenerative world models must go beyond visual realism to achieve geometric consistency, physical plausibility, and functional reliability. We introduce WorldLens, a unified benchmark that evaluates these capabilities across five complementary aspects – from low-level appearance fidelity to high-level behavioral realism. As shown in Figure 2, each aspect is decomposed into fine-grained, interpretable dimensions, forming a comprehensive framework that bridges human perception, physical reasoning, and downstream utility.\n3.[ADDRESS_REMOVED] decomposes the overall generation quality into eight complementary dimensions that assess appearance fidelity, temporal stability, geometric correctness, and semantic smoothness. Together, these dimensions quantify how “faithfully” a model constructs visually and perceptually consistent driving scenes across time and viewpoints.\nG.[ADDRESS_REMOVED] instances, e.g., vehicles and pedestrians. For each generated frame, object regions are cropped using bounding boxes and evaluated with class-specific binary classifiers trained on real data [24]. A high confidence indicates that the generated object visually aligns with its real-world category. This dimension captures localized realism and complements global metrics by focusing on instance-level fidelity.\nG.[ADDRESS_REMOVED]’s identity throughout a generated sequence. Using known track IDs, we extract visual embeddings for each instance from a pretrained ReID model [148, 34] and compute similarity across frames. A high coherence score indicates that objects maintain consistent shape, color, and texture through motion, reflecting stable appearance and identity preservation over time.\nG.[ADDRESS_REMOVED] Consistency evaluates fine-grained temporal stability of object-level semantics and geometry. It uses DINO [12] features to capture texture and spatial details across frames, ensuring that subjects preserve their structure and semantic meaning without flickering or deformation. High consistency reflects the reliable temporal evolution of objects and smooth changes in appearance under motion.\nG.4 Depth Discrepancy measures the smoothness of depth variations across time, capturing geometric coherence. We estimate monocular depth for each frame using Depth Anything V2 [126], coded with colors, and extract corresponding embeddings through DINO v2 [75]. The average feature distance between consecutive frames quantifies the continuity of depth perception. Lower discrepancy indicates geometrically stable and physically plausible scene motion.\nG.5 Temporal Consistency quantifies global frame-to-frame smoothness in a learned appearance space. Each frame is embedded with the CLIP visual encoder [79], and temporal stability is derived from adjacent-frame similarity, jitter suppression, and motion-rate alignment with real videos. High consistency corresponds to temporally stable dynamics without abrupt or unnatural transitions.\nG.6 Semantic Consistency evaluates whether the semantic layout of generated scenes evolves smoothly across time. We employ a pretrained SegFormer [117] to predict frame-wise masks and compute stability across labels, regions, and class distributions. This dimension ensures that generated videos maintain coherent object semantics and scene structures without flickering boundaries or label inconsistencies.\nG.7 Perceptual Discrepancy quantifies the overall perceptual gap between real and generated videos. We extract spatiotemporal embeddings from a pretrained I3D [13] and compute the Fréchet Video Distance [101] between real and generated distributions. A lower discrepancy indicates closer alignment in appearance and motion statistics, reflecting perceptual realism and temporal coherence.\nG.8 Cross-View Consistency measures the geometric and photometric alignment between overlapping regions of adjacent camera views. Using LoFTR [92], we detect feature correspondences between synchronized camera pairs and aggregate their confidence to assess spatial coherence. High consistency indicates better structural alignment and visual continuity across multiple perspectives, ensuring 3D-consistent generation for multi-camera driving systems.\n3.[ADDRESS_REMOVED] decomposes the overall reconstructability into how well a coherent 4D scene can be recovered from generated videos. Each sequence is lifted into a Gaussian Field and re-rendered under both original and novel camera trajectories, testing spatial interpolation, parallax, and view generalization across representative novel-view paths.\nR.1 Photometric Error measures how accurately reconstructed scenes reproduce their input frames. Each generated video is reconstructed into a differentiable 4D representation [17, 50], and re-rendered at original camera poses. Pixel-level similarities, i.e., LPIPS, PSNR, and SSIM, are computed, reflecting stability of appearance and lighting across time. Lower discrepancy indicates more consistent photometric properties for faithful 4D reconstruction.\nR.2 Geometric Discrepancy assesses how well the reconstructed geometry from generated videos aligns with real-world structure. Using identical camera poses, we reconstruct 4D scenes from both generated and ground-truth sequences and compare their rendered depth maps. The Absolute Relative Error (AbsRel) and other related metrics are computed within regions selected by Grounded-SAM 2 [81, 80]. Lower values indicate more plausible depth and consistent surface geometry with real scenes.\nR.3 Novel-View Quality evaluates the perceptual realism of re-rendered frames from unseen camera trajectories. Each reconstructed scene is rendered along novel paths using the same differentiable framework, and visual quality is scored by MUSIQ [49]. A higher score indicates that novel views remain sharp, artifact-free, and visually coherent, demonstrating that the model preserves appearance and illumination consistency beyond training viewpoints.\nR.4 Novel-View Discrepancy quantifies the perceptual gap between novel-view renderings from generated and real reconstructions. Both are rendered under identical camera trajectories, and their distance is measured via FVD [101] on I3D features [13]. Lower discrepancy indicates better generalization to unseen viewpoints, maintaining coherent geometry, appearance, and temporal dynamics in 4D space.\n3.3 Action-Following\nThis aspect evaluates how well the generated worlds support plausible driving decisions when interpreted by pretrained planners, examining whether synthesized scenes provide realistic visual and motion cues that yield real-world-consistent actions. All evaluations are conducted in a generative simulator using custom-designed routes derived from real-world maps against standard benchmarks [9, 10].\nA.1 Displacement Error measures functional consistency between the trajectories predicted from generated and real videos. Using a pretrained end-to-end planner, UniAD [38] or VAD [45], both sequences are used to predict future waypoints, and their mean L2 distance is computed. Lower displacement error indicates that the generated scenes preserve motion cues required for accurate trajectory forecasting.\nA.2 Open-Loop Adherence evaluates how well a pretrained policy performs when operating on generated videos in a generative simulator. In open-loop mode, the policy’s predictions are used purely for evaluation and do not affect the simulated ego-vehicle motion. Following NAVSIM [20], we adopt the Predictive Driver Model Score (PDMS), which aggregates safety, progress, and comfort sub-scores computed over a short simulation horizon. Higher PDMS indicates that the policy exhibits realistic, stable, and safe driving behaviors even when guided solely by generated visual input, reflecting reliable short-term functional realism.\nA.3 Route Completion measures long-horizon navigation stability in closed-loop simulation. It computes the percentage of a predefined route completed before termination due to collision, off-road drift, or timeout. Higher route completion rates signify that generated environments support continuous, physically consistent control over extended trajectories, enabling sustained goal-directed motion.\nA.4 Closed-Loop Adherence integrates both motion quality and task success into a single metric, the Arena Driving Score (ADS) [127]. In the closed-loop mode, the planning decisions of the driving agent directly control the ego’s actions, thereby influencing the simulated environment. ADS multiplies the PDMS and Route Completion scores, rewarding agents that are both safe and successful. A high ADS implies that the planner achieves realistic, collision-free navigation while completing the route effectively, confirming that generated worlds not only look real but also drive real within an autonomous-control loop.\n3.[ADDRESS_REMOVED] evaluates the downstream utility of generated videos by measuring how well 3D perception models pretrained on real data perform when applied to synthetic content. Performance degradation across tasks reflects the realism, fidelity, and transferability of generated scenes.\nD.1 Map Segmentation evaluates whether generated data contain sufficient spatial and semantic cues for top-down BEV mapping. A pretrained BEV map segmentation network [66, 58] predicts semantic maps for each frame, and performance is measured by mean Intersection-over-Union (mIoU). Higher mIoU indicates better structural layout and semantics conducive to accurate BEV reconstruction.\nD.2 3D Object Detection tests whether generated data retains geometric cues essential for perceiving traffic participants. A pretrained BEVFusion [66] is applied to generated frames, and performance is reported using nuScenes Detection Score (NDS) [9]. Higher values indicate that generated scenes support more accurate 3D object localization.\nD.3 3D Object Tracking measures motion consistency and identity information preservation of generated videos across time. A pretrained 3D tracker [22] predicts 3D trajectories from each generated video, and performance is quantified by Average Multi-Object Tracking Accuracy (AMOTA) under the nuScenes protocol [9]. Higher AMOTA reflects more stable temporal dynamics and accurate data association for moving objects from the 3D scene.\nD.4 Occupancy Prediction evaluates whether generated scenes enable accurate 3D reconstruction of spatial geometry and semantics. A frozen SparseOcc [96] predicts voxel-wise scene representations, and performance is measured using RayIoU, which compares semantic agreement along camera rays rather than volumetric overlap. Higher RayIoU indicates more accurate and depth-consistent occupancy estimation, showing that generated videos preserve 3D structural integrity crucial for downstream scene understanding.\n3.[ADDRESS_REMOVED] evaluates alignment with human judgment by assessing how visually authentic, physically coherent, and behaviorally safe the generated videos appear to observers. Each dimension is rated on a ‘’ to ‘’ scale, where higher scores indicate stronger human perceptual fidelity.\nH.1 World Realism measures the overall authenticity and naturalness of generated videos. We evaluate how closely textures, lighting, and motion resemble those in real-world driving footage. Three sub-dimensions are considered: (1) Overall Realism, capturing global scene coherence and natural appearance; (2) Vehicle Realism, assessing vehicle geometry, surface reflectance, and motion stability; and (3) Pedestrian Realism, focusing on body proportion and walking motion consistency. Higher scores indicate scenes that are visually indistinguishable from real recordings.\nH.2 Physical Plausibility evaluates whether the scene obeys intuitive physical and causal principles. It focuses on the continuity of motion, correctness of occlusion order, object contact stability, and illumination consistency across time. Scenes exhibiting teleportation, interpenetration, or inconsistent reflections receive lower ratings, while those maintaining smooth transitions and physically coherent dynamics are scored higher.\nH.3 3D & 4D Consistency measures spatial-temporal coherence of geometry and appearance across frames. It assesses whether reconstructed 3D structures remain stable over time and whether objects preserve their relative positions, orientations, and trajectories. High consistency reflects that generated videos maintain realistic 3D layout and smooth temporal evolution, forming plausible 4D scenes aligned with real-world dynamics.\nH.[ADDRESS_REMOVED] in predictable and risk-free ways consistent with common driving norms. It focuses on short-term interactions among vehicles, pedestrians, and environmental cues, such as adherence to traffic signals, collision avoidance, and stable lane following. Lower scores indicate abrupt or unsafe behaviors (e.g., sudden collisions or unrealistic crossings), whereas higher scores correspond to smooth, lawful, and controlled motion indicative of safe and credible agent behavior.\n4 Human Annotation & Evaluation Agent\n4.1 Annotation Process\nTo establish reliable human supervision for our benchmark, we designed a structured multi-stage annotation pipeline. Ten annotators were divided into two independent groups, each responsible for scoring all videos under the four dimensions defined in Section 3.5. For every video and dimension, the two groups annotated separately; when their ratings diverged, the sample was re-evaluated to ensure consistency. Annotators were presented with four synchronized views: 1generated video, 2semantic mask, 3depth map, and 43D boxes, through the interface shown in Figure 3. To promote consistency and domain understanding, all annotators received detailed documentation with examples illustrating each scoring level. On average, each annotation took approximately two minutes and eight seconds, amounting to over hours. Further implementation details, documentation, and examples are provided in the Appendix.\n4.2 WorldLens-26K: A Diverse Preference Dataset\nTo bridge the gap between human judgment and automated evaluation, we curate a large-scale human-annotated dataset comprising scoring records of generated videos. Each entry includes a discrete score and a concise textual rationale written by annotators, capturing both quantitative assessment and qualitative explanation. The dataset covers complementary dimensions of perceptual quality (section˜3.5). This balanced design ensures comprehensive coverage across spatial, temporal, and behavioral aspects of world-model realism. As shown in Figure 4, the word clouds of textual rationales align closely with their corresponding target dimensions, confirming the validity and interpretability of the collected labels. We envision WorldLens-26K as a foundational resource for training auto-evaluation agents and constructing human-aligned reward or advantage functions for reinforcement fine-tuning of generative world models.\n4.3 WorldLens-Agent: SFT from Human Feedback\nEvaluating generated worlds hinges on human-centered criteria (physical plausibility) and subjective preferences (perceived realism) that quantitative metrics inherently miss, highlighting the necessity of a human-aligned evaluator. To this end, we introduce WorldLens-Agent, a vision-language critic agent trained on WorldLens-26K. Through LoRA-based supervised fine-tuning, we distill human perceptual and physical judgments into a Qwen3-VL-8B [2], enabling it to internalize criteria such as realism, plausibility, and behavioral safety. This provides consistent, human-aligned assessments, offering a scalable preference oracle for benchmarking future world models. Kindly refer to Figure 8 and the Appendix for automatic scoring and rationale generation examples on out-of-distribution videos.\n5 Experiments\nWe comprehensively evaluate representative driving world models across all five aspects defined in WorldLens , covering both quantitative and human-in-the-loop dimensions. Due to space constraints, detailed configurations, metrics, and implementation details are provided in the Appendix.\n5.1 Per-Aspect Evaluations\nGeneration. As summarized in Table 1, all existing models remain notably below the ‘Empirical Max’, indicating substantial room for improving the visual and temporal realism of driving world models. Although DiST-4D [30] achieves the lowest Perceptual Discrepancy, it underperforms OpenDWM [89] in Subject Fidelity and View Consistency, demonstrating that perceptual metrics alone are insufficient for assessing physically coherent scene generation. OpenDWM provides the most balanced overall performance, largely due to large-scale multi-dataset training, while single-dataset models such as MagicDrive [27] and -Scene [128] exhibit limited generalization across all metrics. Notably, conditioned approaches like DiST-4D [30] and DriveDreamer-2 [142] partially overcome this limitation, improving Depth and Cross-View Consistency by – through the use of ground-truth frames. These results highlight that the dataset diversity and conditioning strategies are more critical than perceptual fidelity for achieving reliable, temporally consistent world modeling.\nReconstruction. We assess the spatiotemporal 3D coherence by reconstructing generated videos into 4D Gaussian Fields [17], where floaters and geometric instability directly reveal temporal inconsistency. As shown in Table 1, MagicDrive [27] exhibits the weakest reconstruction, with the highest Photometric Error and Geometric Discrepancy, both over worse than OpenDWM [89]. DreamForge [69] shows similar artifacts, indicating limited 3D Consistency. In contrast, OpenDWM and DiST-4D [30] achieve markedly better reconstruction, reducing photometric and geometric errors by about and producing more structurally coherent sequences. DiST-4D further attains the best Novel-View Quality, likely due to its RGB-D generation design that better preserves depth over time. As illustrated in Figure 5, MagicDrive and DreamForge produce dense floaters and distortions under lateral views, whereas OpenDWM and DiST-4D maintain clean, stable geometry. Overall, the results highlight that the temporal stability and geometric consistency are essential for physically realistic and reconstructable world models.\nAction-Following. As shown in Table 2, we evaluate the functional viability of synthesized environments through closed-loop simulation, where a pretrained planner operates within the “world” of each model. Temporal coherence proves critical, as planning agents rely on multi-frame history and ego-state cues; models with weaker temporal stability achieve the lowest Route Completion rates. A notable finding is the large disparity between open-loop and closed-loop performance. Despite strong open-loop results on Displacement Error and PDMS, all methods collapse under closed-loop conditions, achieving only marginal Route Completion rates. Frequent failures (e.g., collisions, off-road drift) suggest that current synthetic data remain inadequate substitutes for real-world data in high-level control. This highlights a key insight: enhancing the physical and causal realism of generative world models is indispensable for effective closed-loop deployment.\nDownstream Tasks. This aspect directly reflects the practical utility of world models beyond visual realism. As shown in Table 3 and Figure 1, DiST-4D [30] leads by a large margin across all tasks, (i.e., map segmentation, 3D detection, and tracking), averaging – higher than the next best models. DriveDreamer-2 [142] ranks second, particularly excelling in occupancy prediction, highlighting the advantage of temporal conditioning for consistent video generation. In contrast, MagicDrive [27] performs weakest across all tasks, confirming its limited spatiotemporal coherence. Interestingly, despite strong perceptual quality, OpenDWM [89] underperforms in detection () and tracking (), suggesting that large-scale multi-domain training may hinder adaptation to specific dataset distributions. Additional qualitative assessments in Figure 6 further verify our observations. Overall, these results indicate that the temporal conditioning and dataset alignment are critical for task-specific effectiveness for practical usages.\n5.2 Human Preference Alignments\nSubjective Evaluations. Since not all aspects of world modeling can be captured by quantitative metrics, we conducted a human evaluation focusing on World Realism, Physical Plausibility, 3D & 4D Consistency, and Behavioral Safety. As shown in Figure 7, overall scores remain modest (on average ‘’‘’ out of ‘’), revealing that current world models are far from human-level realism. DiST-4D [30] achieves the most balanced scores across all dimensions, leading in physical plausibility (‘’) and behavioral safety (‘’). OpenDWM [89] attains the highest realism (‘’) but slightly lower physical consistency, while MagicDrive [27] ranks lowest overall, reflecting poor coherence. Interestingly, World Realism and Consistency scores correlate strongly, suggesting that human-perceived realism is tightly coupled with geometric and temporal stability. Overall, these results underscore the necessity of human-in-the-loop evaluation to complement quantitative benchmarks and provide a holistic assessment of world-model quality.\nHuman-Agent Alignments. To assess the generalizability of our automatic evaluator, WorldLens-Agent, we conduct a zero-shot test on videos generated by Gen3C [83], as shown in Figure 8. The agent’s predicted scores exhibit strong alignment with human annotations across all evaluated dimensions, confirming its ability to capture nuanced subjective preferences. Beyond numerical agreement, the textual rationales generated by the agent closely mirror those written by human annotators, demonstrating both score-level consistency and interpretive coherence. These results highlight the effectiveness of leveraging human-annotated perception data to train scalable, explainable, and reproducible evaluative agents for future world-model benchmarking.\n5.3 Insights & Discussions\nComprehensive Evaluations are Crucial. No single world model excels in all aspects (Figure 1): DiST-4D performs best in geometry and novel-view metrics, OpenDWM leads in photometric fidelity, and DriveDreamer-[ADDRESS_REMOVED] depth accuracy. This divergence shows that visual realism, geometric consistency, and downstream usability are complementary rather than interchangeable, highlighting the necessity of multi-dimensional benchmarking.\nPerceptual Quality Does Not Imply Usability. Models with strong perceptual scores (e.g., OpenDWM) may underperform on downstream tasks. Despite its visual fidelity, OpenDWM scores 30% lower than DiST-4D in 3D detection, indicating that large-scale, multi-domain training can hinder adaptation to task-specific distributions. Hence, aligning generated data with the target domain is more crucial than perceptual realism for effective downstream use.\nGeometry Awareness Enables Physical Coherence. The superior reconstruction and novel-view performance of DiST-4D stem from its RGB-D generation and decoupled spatiotemporal diffusion, which jointly model temporal forecasting and spatial synthesis. This shows that geometry-aware supervision significantly improves the physical realism and reconstructability of generated scenes.\nJoint Optimization of Appearance and Geometry. The discrepancy between photometric (LPIPS/PSNR) and geometric metrics (Abs Rel) reveals that current models often treat texture and structure as independent objectives. Geometry-aware supervision stabilizes depth but blurs details, while appearance-driven training sharpens textures yet breaks spatial consistency. A unified formulation that jointly optimizes appearance and geometry through spatiotemporal regularization yields consistent reconstruction.\nGuidelines for Future World Model Design. Key principles emerge for developing physically grounded world models: 1) Prioritize geometry as a core objective: explicit depth prediction or supervision enhances both reconstruction fidelity and downstream perception; 2) Stabilize foreground dynamics: consistent geometry is essential for reliable motion disentanglement; 3) Ensure autoregressive resilience: enforcing cross-view and temporal consistency mitigates drift and structural artifacts, while training with self-forcing [40, 18] or streaming diffusion [51] enhances robustness against compounding errors in closed-loop generation, which is crucial for long-horizon stability. Overall, robust world models stem from the joint optimization of appearance, geometry, and task adaptability, advancing from visual realism toward physical reliability.\n6 Conclusion\nWe presented WorldLens , a full-spectrum benchmark that evaluates generative world models across perception, geometry, function, and human alignment perspectives. Through five complementary evaluation aspects and over twenty standardized metrics, it offers a unified protocol for measuring both physical and perceptual realism. Together with WorldLens-26K and WorldLens-Agent, our framework establishes a scalable, interpretable foundation for benchmarking future world models – guiding progress toward systems that not only look real but also behave realistically.\nAppendix\n[ADDRESS_REMOVED] 1: Generation\nIn this section, we detail the metrics used to evaluate the quality of generation of driving world models. This aspect assesses the overall realism, coherence, and physical plausibility of generated driving videos, capturing how well a model reconstructs the spatiotemporal structure of real-world scenes.\n7.[ADDRESS_REMOVED] Fidelity\n7.1.[ADDRESS_REMOVED] instances, such as vehicles and pedestrians, that appear in generated driving videos. It focuses on assessing whether each synthesized object visually resembles its real-world counterpart in both appearance and semantic attributes. By isolating individual instances, this metric emphasizes fine-grained visual fidelity that global perceptual measures may overlook, providing an object-centric view of generation quality.\n7.1.2 Formulation\nFor a generated video with bounding boxes , we crop object patches . Let denote the evaluated object categories (e.g., vehicle, pedestrian), and be a pretrained binary classifier for class outputting confidence that patch looks real for that class. Aggregating across all objects, frames, videos, and classes yields the overall Subject Fidelity score:\nA higher score indicates that generated objects are both visually convincing and semantically consistent with their intended categories. Models achieving high fidelity tend to produce realistic textures, shapes, and colors, even under varying viewpoints and lighting conditions. This metric thus complements global measures like FVD or LPIPS by focusing on localized realism at the instance level, offering insights into whether the generated world contains physically believable and semantically meaningful entities.\n7.1.3 Implementation Details\nWe use class-specific confidence scores for evaluation. Pedestrian crops are classified using a pedestrian classifier pretrained on several commonly used pedestrian-datasets [144, 110, 19, 76], while vehicle crops are classified with a ViT-B/16 model (‘google/vit-base-patch16-224‘) [113] pretrained on ImageNet-21k ( million images, classes) [21]. Category grouping is determined by regex-based label matching against the model’s id2label. Images are resized to and normalized before inference. For each tracklet, we average the classification confidence of all selected frames, and report the mean confidence as the final score.\n7.1.4 Examples\nFigure [ADDRESS_REMOVED] Fidelity.\n7.1.5 Evaluation & Analysis\nTable [ADDRESS_REMOVED] Fidelity.\n7.[ADDRESS_REMOVED] Coherence\n7.2.[ADDRESS_REMOVED]’s visual identity across consecutive frames within a generated sequence. It captures whether the same entity – such as a specific car or pedestrian – maintains consistent appearance attributes, including color, texture, and shape, over time. This metric assesses not only visual continuity but also the preservation of object identity, which is crucial for generating physically plausible and temporally coherent scenes for autonomous driving applications.\n7.2.2 Formulation\nFor each generated video , the conditioning provides bounding boxes and associated track IDs . Object patches are cropped as for object track . A frozen ReID encoder extracts -normalized embeddings:\nThe dataset-level Subject Coherence is computed as the mean cosine similarity between consecutive embeddings of the same tracked object, aggregated over all tracks, frames, and videos:\nwhere is the number of track IDs in video and the number of frames where object appears. A high score reflects consistent and temporally stable object generation, indicating that the model preserves identity-related features despite changes in position, viewpoint, or lighting. In contrast, a low score often signals flickering textures, shape distortions, or identity switches between frames.\nThis metric thus serves as a sensitive indicator of temporal realism, distinguishing models that produce temporally coherent scenes from those limited to frame-wise synthesis.\n7.2.[ADDRESS_REMOVED] Coherence using embeddings extracted from the Cross-Video ReID model of Zuo et al. [148]. Frames are filtered using confidence thresholds of for vehicles and for pedestrians before similarity computation. The final score is a combination of both sub-metrics.\n7.2.4 Examples\nFigure [ADDRESS_REMOVED] Coherence.\n7.2.5 Evaluation & Analysis\nTable [ADDRESS_REMOVED] Coherence.\n7.[ADDRESS_REMOVED] Consistency\n7.3.[ADDRESS_REMOVED]-level semantics and structural details. It focuses on fine-grained appearance and geometric regularity through DINO features [12], evaluating whether dynamic subjects maintain consistent texture, shape, and structure over time. High scores indicate that the model preserves the semantic identity and visual integrity of objects throughout motion, avoiding flickering or deformation.\n7.3.2 Formulation\nFor each generated video and its paired ground-truth , we extract -normalized DINO embeddings: , , and , where denotes the frozen DINO feature extractor. To quantify temporal stability, we compute three complementary terms:\n-\n•\nAdjacent-Frame Smoothness:\nwhich measures the average cosine similarity between consecutive frame embeddings.\n-\n•\nTemporal Jitter Index (TJI):\nwhich measures normalized second-order fluctuations (lower is smoother).\n-\n•\nMotion-Rate Similarity (MRS):\nwhich aligns the per-frame feature motion magnitude with that of the ground-truth sequence.\nThe overall Subject Consistency score integrates these terms:\n7.3.[ADDRESS_REMOVED] frame-wise features using DINO ViT-B/16 [12]. These embeddings are used to compute adjacent-frame similarity, temporal jitter, and motion alignment against the corresponding ground-truth videos.\n7.3.4 Examples\nFigure [ADDRESS_REMOVED] Consistency.\n7.3.5 Evaluation & Analysis\nTable [ADDRESS_REMOVED] Consistency.\n7.4 Depth Discrepancy\n7.4.1 Definition\nDepth Discrepancy quantifies the temporal stability of depth representations inferred from generated video sequences. In natural driving scenes, the apparent depth of foreground and background objects evolves smoothly with camera motion, whereas inconsistent generation often introduces discontinuous jumps in predicted depth. This metric captures such instability by measuring temporal variation in depth embeddings extracted from consecutive frames, providing a geometric complement to perceptual fidelity metrics.\n7.4.2 Formulation\nFor a generated video , we estimate per-frame depth maps using a monocular depth estimator :\nEach depth map is RGB-encoded by a fixed colormap and processed by a pretrained visual encoder to obtain global embeddings:\nTemporal variation in depth representation is then measured by the mean L2 distance between consecutive embeddings:\nFinally, the dataset-level Depth Discrepancy can be calculated as follows:\nLower indicates smoother, more physically consistent depth evolution across time, reflecting stronger temporal geometric stability in the generated videos.\n7.4.3 Implementation Details\nDepth maps for both generated and ground-truth videos are obtained using Video DepthAnything [15]. The predicted depths are directly used to compute the per-frame depth discrepancy.\n7.4.4 Examples\nFigure 12 provides typical examples of videos with good and bad quality in terms of Depth Discrepancy.\n7.4.5 Evaluation & Analysis\nTable 7 provides the complete results of models in terms of Depth Discrepancy.\n7.5 Temporal Consistency\n7.5.1 Definition\nTemporal Consistency quantifies the frame-to-frame stability of generated videos in a learned appearance space. Using a frozen CLIP encoder, this metric captures whether visual representations evolve smoothly over time without abrupt changes or flickering. It measures: (1) adjacent-frame smoothness, (2) suppression of high-frequency temporal jitter, and (3) alignment of motion magnitudes with real sequences. Together, these components evaluate whether generated videos exhibit physically coherent and temporally realistic dynamics.\n7.5.2 Formulation\nFor each generated video and its paired ground-truth , we extract -normalized CLIP embeddings as follows:\nFollow the temporal statistics calculations in Subject Consistency (Eq. 7.3), the adjacent-frame smoothness, jitter suppression, and motion-rate alignment are applied in this CLIP space. Combining these components, the per-video score is defined as:\nThe dataset-level metric averages per-video scores:\nwith and . By construction, and .\nA high score indicates that appearance features change gradually across frames, producing smooth motion and physically coherent dynamics. Low scores correspond to flickering, abrupt illumination shifts, or motion discontinuities. This metric captures the degree to which generated sequences maintain continuity in both content and motion, serving as a robust proxy for temporal realism in driving videos.\n7.5.3 Implementation Details\nTemporal Consistency is evaluated using frame-wise features from CLIP ViT-B/32 [79] with an input resolution of . The normalized embeddings are used to derive adjacent-frame similarity, a temporal jitter index, and motion alignment between generated and ground-truth videos.\n7.5.4 Examples\nFigure 13 provides typical examples of videos with good and bad quality in terms of Temporal Consistency.\n7.5.5 Evaluation & Analysis\nTable 8 provides the complete results of models in terms of Temporal Consistency.\n7.6 Semantic Consistency\n7.6.1 Definition\nSemantic Consistency assesses the temporal stability of scene semantics in generated videos, ensuring that the underlying segmentation layout evolves smoothly over time. Using a frozen semantic segmentation model , this metric quantifies how consistently pixel-wise labels, region structures, and global class distributions are preserved between consecutive frames.\n7.6.2 Formulation\nFor each generated video , we obtain frame-wise segmentation masks: . Temporal semantic stability is quantified by three complementary components:\nLabel Flip Rate (LFR) measures how rarely interior pixels (after class-wise morphological erosion) change their semantic label between consecutive frames. For class , let be the eroded interior region. The flip ratio is the fraction of pixels in whose labels differ in . The per-video LFR score averages these values across classes and time, then normalizes: .\nSegment Association Consistency (SAC) measures how consistently connected semantic regions persist over time. For each class , connected components in and are matched by Hungarian assignment over IoU. The score is the pixel-weighted mean IoU of the matched region pairs: , where is the optimal region matching.\nClass Distribution Stability (CDS) compares frame-level class histograms. Let be the normalized histogram of frame . Global distribution shift is quantified by the Jensen–Shannon divergence: .\nEach component is normalized to . The final Semantic Consistency score is a weighted combination:\nwith . A high score signifies that drivable areas, lane boundaries, and object classes remain stable under temporal changes.\n7.6.3 Implementation Details\nWe obtain frame-wise semantic maps using the panoptic segmentation model from OpenSeeD [138]. The predicted segments are then converted to label masks via a fixed color palette and used to compute the temporal semantic consistency score.\n7.6.4 Examples\nFigure 14 provides typical examples of videos with good and bad quality in terms of Semantic Consistency.\n7.6.5 Evaluation & Analysis\nTable 9 provides the complete results of models in terms of Semantic Consistency.\n7.7 Perceptual Discrepancy\n7.7.1 Definition\nPerceptual Discrepancy evaluates how closely the distribution of generated videos matches that of real ones in a learned video, semantic feature space, typically extracted by a pretrained I3D network [95] trained on Kinetics [48].\nThis metric captures both appearance realism and short-range temporal dynamics beyond framewise image-based metrics (e.g., FID), thus reflecting the overall perceptual quality of the synthesized sequences. It is reported as a single scalar, where a lower score indicates higher perceptual similarity to real videos.\n7.7.2 Formulation\nLet the real and generated video sets be and . Each video is encoded into a -dimensional feature vector using a fixed video encoder :\nLet and be the empirical means and covariances of the feature sets and , respectively. Following the Fréchet formulation, the Perceptual Fidelity score (equivalent to the Fréchet Video Distance, FVD) is defined as follows:\nA lower indicates that the generated distribution is perceptually closer to the real distribution .\nPerceptual Discrepancy serves as a global perceptual indicator of visual and temporal realism. By comparing distributions in a semantically informed video embedding space, it evaluates not only static appearance but also dynamic motion smoothness and coherence. A low score indicates that the generative model produces sequences with authentic spatial structures, plausible dynamics, and consistent motion statistics, while a high score reveals perceptual drift or domain mismatch.\nThis metric thus complements fine-grained evaluations by providing an overarching measure of distributional fidelity in world-model generation.\n7.7.3 Implementation Details\n7.7.4 Examples\nFigure 15 provides typical examples of videos with good and bad quality in terms of Perceptual Discrepancy.\n7.7.5 Evaluation & Analysis\nTable 10 provides the complete results of models in terms of Perceptual Discrepancy.\n7.8 Cross-View Consistency\n7.8.1 Definition\nCross-View Consistency evaluates the geometric and photometric coherence across overlapping regions between adjacent camera views in a multi-view driving scene. A spatially consistent generation should ensure that content observed from different cameras remains structurally aligned and visually coherent, faithfully representing the same physical world from multiple perspectives. This property is critical for autonomous driving, as consistent multi-view generation reflects an accurate understanding of shared 3D geometry and scene semantics.\nWe quantify this consistency by computing the mean accumulated confidence of feature correspondences between overlapping edge regions of adjacent camera pairs using a pretrained local feature matcher. Higher confidence indicates better geometric and appearance alignment across views.\n7.8.2 Formulation\nFor each generated scene with synchronized views and frames, a frozen LoFTR matcher produces correspondences with confidence between every adjacent camera pair at frame . The overall Cross-View Consistency score averages all confidences across pairs, frames, and videos:\nHigher indicates stronger geometric and appearance alignment between adjacent camera views.\nA high Cross-View Consistency score signifies that the generated multi-view scene maintains coherent 3D geometry and visual appearance across cameras, implying stable spatial reasoning and accurate scene composition. Conversely, low scores reveal misalignments such as perspective drift, inconsistent object boundaries, or mismatched illumination across views.\nThis metric thus serves as a key indicator of multi-camera integrity, linking the generative model’s visual realism to its geometric understanding of the physical world.\n7.8.3 Implementation Details\nThe Cross-View Consistency score is computed by extracting frame-wise sparse correspondences using the pretrained LoFTR local feature matcher [92]. Matched keypoints across views are used to assess geometric alignment between generated and ground-truth videos.\n7.8.4 Examples\nFigure 16 provides typical examples of videos with good and bad quality in terms of Cross-View Consistency.\n7.8.5 Evaluation & Analysis\nTable 11 provides the complete results of models in terms of Cross-View Consistency.\n[ADDRESS_REMOVED] 2: Reconstruction\nThis aspect assesses the reconstructability of generated videos. Given a reconstructed neural 4D representation built from each generated video, we evaluate both its internal fidelity and its rendering performance from novel viewpoints. A high-quality generation should preserve temporally coherent geometry, appearance, and illumination that jointly support faithful 4D reconstruction. We employ differentiable 4D reconstruction to optimize scene geometry and radiance from the generated sequences, then re-render the reconstructed model under both original and unseen camera poses.\n8.1 Photometric Discrepancy\n8.1.1 Definition\nPhotometric Discrepancy quantifies how accurately the 4D scene reconstructed from a generated video can reproduce its observed frames. Each generated sequence is first converted into a neural radiance field using a differentiable pipeline based on 4D Gaussian Splatting or NeRF-based [71] video reconstruction. The reconstructed model is then re-rendered from the same camera poses as the input frames, and pixel-wise fidelity is evaluated using standard image quality metrics such as PSNR, SSIM [109], and LPIPS [140].\n8.1.2 Formulation\nLet denote the 4D reconstruction function that produces a radiance field from a generated video . Rendering this field at the input camera poses yields re-rendered frames: , where is the camera pose of frame . Photometric fidelity is measured by the mean Learned Perceptual Image Patch Similarity (LPIPS) between the reconstructed and original frames:\nHigher indicates that the reconstructed radiance fields preserve fine-grained appearance details consistent with the generated frames.\n8.1.3 Implementation Details\nWe follow the OmniRe [17] preprocessing pipeline and default configuration on nuScenes [9], using the same -camera setup. Each generated clip is treated as a short multi-view sequence ( Hz, frames per camera at resolution). For each clip, we optimize a single 4D Gaussian field for steps, adopting OmniRe’s static- and dynamic-node Gaussian initializations [17] as well as its batch size, ray-sampling strategy, loss weights, and learning-rate schedule. After training, we render all training views and evaluate PSNR, SSIM, and LPIPS averaged over all frames and cameras.\n8.1.4 Examples\nFigure 17 provides typical examples of videos with good and bad quality in terms of Photometric Discrepancy.\n8.1.5 Evaluation & Analysis\nTable 12 provides the complete results of models in terms of Photometric Discrepancy.\n8.2 Geometric Discrepancy\n8.2.1 Definition\nGeometric Discrepancy evaluates how faithfully the geometry encoded in a generated video can be recovered after reconstruction. For each generated video and its paired ground truth, we reconstruct two 4DGS models using identical camera poses and training parameters, then render per-frame depth maps for both reconstructions. Depth consistency is measured using the Absolute Relative Error (Abs Rel) computed on regions defined by Grounded-SAM 2 [80] masks that isolate road surfaces and foreground objects.\n8.2.2 Formulation\nLet denote the 4D reconstruction function. For each generated video and ground truth , we obtain two reconstructed fields and . At each training pose , the corresponding depth maps are rendered as:\nLet be the Grounded-SAM 2 mask selecting conditioned pixels. The overall Geometric Accuracy score averages the masked AbsRel error over all frames and videos:\nLower indicates that the reconstructed geometry from generated videos is more consistent with the ground-truth scene structure.\n8.2.[ADDRESS_REMOVED] shares the same training setup as the photometric discrepancy. The main difference is in the rendering and metric computation. For each clip, we render per‑pixel depth from the learned 4D Gaussian field for all training views using the default Gaussian rasterizer (GSplat [134]) as in OmniRe [17], configured in the “RGB+ED” mode that outputs both color and Euclidean depth along each camera ray. To obtain fair and semantically meaningful depth metrics, we construct evaluation masks from ground‐truth images using Grounded SAM 2 [80], extracting the union of the road and vehicle regions. Depth errors, e.g., Abs Rel and Root Mean Squared Error (RMSE), are then computed only within these masked pixels by comparing with the depth rendered by the GT‑trained Gaussian field. We also report the threshold accuracy metrics (, , ). Per‑clip scores are obtained by averaging over all frames and cameras of the clip.\n8.2.4 Example\nFigure 18 provides typical examples of videos with good and bad quality in terms of Geometric Discrepancy.\n8.2.5 Evaluation & Analysis\nTable 13 provides the complete results of models in terms of Geometric Discrepancy.\n8.3 Novel-View Quality\n8.3.1 Definition\nNovel View Quality (NVQ) assesses the perceptual quality of rendered frames from unseen camera trajectories, complementing Novel View Fidelity by focusing on frame-level realism rather than distributional similarity. For each novel-view trajectory, we render novel-view videos from reconstructed radiance fields and evaluate the perceptual quality of each frame using the pretrained MUSIQ model [49]. The novel-view trajectories are:\n-\n•\n“front_center_interp”, which smoothly interpolates along the original front-center (ID 0) camera path by selecting four key poses at indices , , , and , and generating intermediate 4×4 poses through linear translation and spherical linear interpolation (Slerp) of orientations.\n-\n•\n“s_curve”, which constructs an S-shaped trajectory by traversing five key poses from front-left (ID 1), front-center (ID 0), and front-right (ID 2) cameras, at indices , , , , and , yielding a smooth left–center–right–center motion.\n-\n•\n“lateral_offset”, which generates a parallel-view sequence by shifting each front-camera pose (ID 0) laterally by a fixed offset along its local axis while preserving orientation, followed by temporal resampling through linear and Slerp interpolation. All trajectories are resampled to a fixed target length.\n8.3.2 Formulation\nGiven the re-rendered novel-view videos under any of the novel-view settings, we compute frame-level perceptual quality scores via the pretrained image-quality assessor .\nEach frame receives a quality score , and the overall dataset-level Novel View Quality is obtained by averaging across all frames and videos:\nHigher indicates better perceptual quality of novel-view renderings, reflecting sharper appearance, fewer artifacts, and more realistic content across unseen trajectories.\n8.3.3 Implementation Details\nWe render videos from four novel viewpoints using the Gaussian Fields trained by each world model, following the definition provided in Section 8.3.1, where the lateral offset is set to 1 m. Novel-view image quality is assessed using the pretrained MUSIQ model [49]. Each rendered novel-view video is processed frame-by-frame (resized to a maximum spatial dimension of pixels), and the MUSIQ scores are averaged across all frames and videos within each view condition.\n8.3.4 Examples\nFigure 19 provides typical examples of videos with good and bad quality in terms of Novel-View Quality.\n8.3.5 Evaluation & Analysis\nTable 14 provides the complete results of models in terms of Novel-View Quality.\n8.4 Novel-View Discrepancy\n8.4.1 Definition\nNovel-View Discrepancy measures the perceptual realism of newly rendered videos under unseen camera trajectories reconstructed from generated scenes.\nGiven the reconstructed neural radiance field of each generated video, we render novel-view sequences at held-out camera poses and compare them against ground-truth novel-view renderings of the corresponding real scenes.\n8.4.2 Formulation\nLet and denote the reconstructed radiance fields from the generated and ground-truth videos, respectively. Rendering each field along a novel trajectory yields two new video sequences:\nWe compute the Fréchet Video Distance (FVD) between the distributions of generated and ground-truth novel-view videos using Eq. (7), where the feature extractor (I3D on Kinetics) remains the same.\nThe dataset-level Novel View Fidelity is thus defined as:\nLower indicates higher perceptual fidelity of the reconstructed scenes when viewed from unseen camera trajectories.\n8.4.3 Implementation Details\n8.4.4 Example\nFigure 20 provides typical examples of videos with good and bad quality in terms of Novel-View Discrepancy.\n8.4.5 Evaluation & Analysis\nTable 15 provides the complete results of models in terms of Novel-View Discrepancy.\n[ADDRESS_REMOVED] 3: Action-Following\nIn this section, we evaluate the Action-Following capability of driving world models, which reflects how well the generated videos preserve the functional cues necessary for downstream decision-making and control. Here, we assess the functional alignment between generated content and real-world driving behavior. Specifically, we examine how the visual information synthesized influences an end-to-end planning agent in both open-loop and closed-loop simulation settings. A model with strong action-following ability should not only generate visually convincing scenes but also guide a pretrained planner to produce trajectories and control actions that are consistent with those derived from real-world videos.\n9.1 Displacement Error\n9.1.1 Definition\nDisplacement Error (L2) evaluates the functional consistency of generated videos on the downstream task of motion planning. Instead of measuring perceptual realism or pixel-level accuracy, this metric assesses whether a generated video can serve as a reliable input for an end-to-end planner. It measures how closely the predicted trajectory inferred from a generated video aligns with the trajectory predicted from the corresponding ground-truth video. A lower displacement error indicates that the generated sequence preserves the semantic and motion cues necessary for robust trajectory forecasting, demonstrating that it is not only visually plausible but also functionally faithful to real-world driving dynamics.\n9.1.2 Formulation\nWe employ a pretrained end-to-end planning network to predict trajectories from both generated and ground-truth videos. Given paired sequences and , the model produces corresponding planned trajectories\nwhere each trajectory contains future waypoints in 2D ground-plane coordinates. The Displacement Error is computed as the mean L2 distance between corresponding waypoints:\nLower indicates that the generated videos induce planning behaviors that are more consistent with those derived from real-world observations, reflecting higher functional fidelity.\n9.1.[ADDRESS_REMOVED] the Displacement Error evaluation on the official nuScenes validation set, which consists of 150 diverse driving scenes. For each test case, the driving world model generates a video sequence conditioned on the initial context. These synthesized videos are then used as input for UniAD [38], a state-of-the-art end-to-end planning network, to infer future ego-motion trajectories. Following the standard protocol, we extract the planned trajectory for a horizon of second (covering the immediate future waypoints). The Displacement Error is calculated as the L2 distance between the trajectory predicted from the generated video and the trajectory predicted from the ground-truth video. This metric strictly isolates the impact of visual generation quality on downstream perception and planning accuracy in an open-loop setting.\n9.1.4 Examples\nFigure 21 provides typical examples of videos with good and bad quality in terms of Displacement Error.\n9.1.5 Evaluation & Analysis\nTable 16 provides the complete results of models in terms of Displacement Error.\n9.2 Open-Loop Adherence\n9.2.1 Definition\nOpen-Loop Adherence evaluates the functional reliability of generated videos by measuring how well an end-to-end driving policy can perform when operating on the generated input in a non-reactive simulation environment. Following NAVSIM [20], we use the Predictive Driver Model Score (PDMS) to quantify adherence between the policy behavior induced by generated videos and that observed under real data.\n9.2.2 Formulation\nGiven a pretrained planner and its predicted trajectory from a generated video , we simulate the resulting ego motion over a fixed horizon (e.g., ) in a non-reactive setting where other agents follow their recorded trajectories. At each timestep, sub-scores are computed for: no collision (NC), drivable-area compliance (DAC), ego progress (EP), time-to-collision (TTC), and comfort (C). Penalties (NC, DAC) suppress inadmissible behaviors, while the remaining terms are averaged with fixed weights. The PDMS is defined as:\nwith default weights and as in [20]. We report the dataset-level score as the mean PDMS across all evaluated videos:\nHigher indicates stronger alignment between generated and real scenes in terms of functional behavior.\n9.2.3 Implementation Details\nWe support two map environments, singapore-onenorth and boston-seaport, aligned with the DriveArena platform [127]. A total of five simulation sequences are defined for validation, enabling the evaluation of driving agents in both open-loop and closed-loop modes. In our implementation, the traffic flow engine [111] operates at a frequency of Hz, while the control signals are set to Hz. Every simulation seconds, the 2D traffic flow engine updates its state and renders multi-view layouts as conditions for the video generation model. Video generation models use the last frames as reference images to generate images, which are subsequently resized to to serve as input for the driving agent.\n9.2.4 Examples\nFigure 22 provides typical examples of videos with good and bad quality in terms of Open-Loop Adherence.\n9.2.5 Evaluation & Analysis\nTable 17 provides the complete results of models in terms of Open-Loop Adherence.\n9.3 Route Completion\n9.3.1 Definition\nRoute Completion (RC) measures the ability of an autonomous driving agent to complete a predefined navigation route in closed-loop simulation. It quantifies the percentage of the total planned route distance successfully traveled by the ego agent before simulation termination (e.g., collision, off-road, or timeout). Higher RC values indicate better long-horizon stability and control consistency, reflecting how well the generated video enables the policy to sustain safe driving behavior throughout the route.\n9.3.2 Formulation\nLet denote the total length of the planned route, and the distance actually traveled by the ego agent before termination. Following [127, 20], Route Completion is defined as the ratio between the completed and total distances:\nWe report the dataset-level metric as the mean RC across all evaluated closed-loop rollouts:\nHigher indicates that the generated scenes enable the planner to complete longer portions of the route, implying greater action stability and environmental consistency.\n9.3.3 Implementation Details\nDifferent from the open-loop evaluation (Displacement Error), both Route Completion and Closed-Loop Adherence are evaluated in a fully reactive closed-loop mode. In this setting, the ego-vehicle’s trajectory is not determined by pre-recorded logs but is driven by the agent’s decisions.\nSpecifically, the planning agent processes the video generated by the world model, outputs a control signal, and this signal updates the ego-vehicle’s state within the simulator.\nThe world model then generates the next frame based on this new state, creating a continuous feedback loop. A simulation episode continues until one of the following termination criteria is met:\n-\n1.\nCompletion: The agent successfully reaches the destination and finishes the predefined route.\n-\n2.\nFailure: The simulation is terminated early due to safety-critical infractions, specifically collision with other objects or driving off-road (exiting the drivable area).\nThis setup evaluates the ability of the generative driving world model to support long-horizon consistency and error-free decision-making.\n9.3.4 Examples\nFigure 23 provides typical examples of videos with good and bad quality in terms of Route Completion.\n9.3.5 Evaluation & Analysis\nTable 18 provides the complete results of models in terms of Route Completion.\n9.4 Closed-Loop Adherence\n9.4.1 Definition\nClosed-Loop Adherence measures the overall driving performance of an autonomous agent in a closed-loop simulation. It is represented by the Arena Driving Score (ADS) [127], which jointly accounts for both driving quality and task completion.\nWhile the PDMS score reflects the safety, comfort, and stability of the predicted trajectory, the Route Completion (RC) measures how much of the planned route is successfully finished without failure. The multiplicative formulation ensures that an agent must be both competent (high PDMS) and consistent (high RC) to achieve a strong overall score. Agents that drive perfectly but crash early, or complete the route with poor motion quality, will both be penalized accordingly.\n9.4.2 Formulation\nGiven the PDMS and RC metrics defined in (9.2.2) and (9.3.2), the Arena Driving Score (ADS) is computed as follows:\nwhere denotes route completion. For a dataset of generated videos , the final closed-loop adherence is reported as the mean ADS across all evaluated driving episodes:\nHigher indicates that the generated videos yield planners capable of both safe and complete driving behavior in closed-loop simulation.\n9.4.3 Implementation Details\nClosed-Loop Adherence shares the same experiment environment with Route Completion. The implementation details can be found in Section 9.3.\n9.4.4 Examples\nFigure 24 provides typical examples of videos with good and bad quality in terms of Closed-Loop Adherence.\n9.4.5 Evaluation & Analysis\nTable 19 provides the complete results of models in terms of Closed-Loop Adherence.\n[ADDRESS_REMOVED] 4: Downstream Task\nIn this section, we evaluate the downstream task utility of generated videos by assessing how well pretrained perception models perform when applied to synthetic data. Rather than measuring visual realism or temporal stability directly, this aspect examines whether a generative world model can produce data that is useful for real-world perception tasks. Specifically, we test four representative downstream tasks that span spatial understanding, object reasoning, and 3D scene interpretation. For each task, a perception model is pretrained on the corresponding ground-truth dataset and then evaluated on videos generated by the world model. Performance degradation relative to the ground truth reflects the distribution gap introduced by generation.\n10.1 Map Segmentation\n10.1.1 Definition\nBEV (Bird’s-Eye-View) Map Segmentation evaluates whether individual generated frames contain sufficient spatial and semantic cues for top-down mapping. A pretrained perception network takes each generated frame as input and predicts a BEV semantic map, which is compared with the corresponding ground-truth annotation using mean Intersection-over-Union (mIoU). Higher scores indicate that the generated frames preserve structural layout and scene semantics conducive to reliable map inference.\n10.1.2 Formulation\nFor each generated frame , the pretrained model predicts a BEV map: and , and denotes the corresponding ground-truth BEV annotation. The per-frame mean IoU is computed as:\nThe dataset-level Map Segmentation score averages over all frames and videos, that is:\nwhere is the number of BEV categories and the BEV map resolution.\n10.1.3 Implementation Details\n10.1.4 Examples\nFigure 25 provides typical examples of videos with good and bad quality in terms of Map Segmentation.\n10.1.5 Evaluation & Analysis\nTable 20 provides the complete results of models in terms of Map Segmentation.\n10.2 3D Object Detection\n10.2.1 Definition\n3D Object Detection evaluates whether generated frames preserve the geometric and motion cues necessary for accurate perception of traffic participants.\nA pretrained detector , trained on ground-truth data, is applied to each generated frame to predict 3D bounding boxes with category, position, scale, and velocity information. Following the nuScenes detection protocol [9], detections are compared against ground-truth boxes to compute mean Average Precision (mAP) and the consolidated nuScenes Detection Score (NDS).\nHigher mAP and NDS indicate that the generated data retains faithful 3D spatial structure and dynamic cues consistent with real-world scenes.\n10.2.2 Formulation\nFor each frame , the pretrained detector predicts a set of 3D bounding boxes:\nPer-frame detection metrics (mAP and NDS) are computed following [58, 66] using standard matching and error terms. The dataset-level 3D detection score averages these values across all generated frames:\nHigher (and mAP) indicates that the generated frames support more accurate 3D reasoning and reliable downstream perception for autonomous driving.\n10.2.3 Implementation Details\nThe 3D detection evaluation uses the same pretrained BEVFusion model as in Section 10.1, with its detection head producing 3D bounding boxes on the nuScenes BEV range and a grid. Predicted boxes are evaluated against ground-truth annotations using standard nuScenes 3D detection metrics.\n10.2.4 Examples\nFigure 26 provides typical examples of videos with good and bad quality in terms of 3D Object Detection.\n10.2.5 Evaluation & Analysis\nTable 21 provides the complete results of models in terms of 3D Object Detection.\n10.3 3D Object Tracking\n10.3.1 Definition\n3D Object Tracking evaluates whether generated videos preserve consistent object motion and identity information that supports temporal data association. A pretrained tracker , trained on ground-truth sequences, is applied to each generated video to estimate 3D trajectories of dynamic objects.\nFollowing the nuScenes tracking protocol [9], tracking performance is measured using the Average Multi-Object Tracking Accuracy (AMOTA), which integrates precision, recall, and association quality across recall thresholds. Higher AMOTA values indicate that the generated videos exhibit realistic temporal dynamics, enabling stable object tracking over time.\n10.3.2 Formulation\nFor each generated video , the tracker predicts a set of object trajectories:\nand denotes the corresponding ground-truth trajectories. Tracking accuracy is evaluated using the official nuScenes metrics [9], including MOTA and AMOTA, where higher scores indicate more reliable data association and motion continuity.\nThe dataset-level 3D tracking metric averages per-video AMOTA over all generated sequences:\nHigher indicates that generated videos maintain realistic and temporally coherent object motion, supporting accurate long-term tracking.\n10.3.3 Implementation Details\nWe evaluate the 3D object tracking performance using the pretrained camera-only ADA-Track [22], following its official nuScenes configuration. The tracker is run directly on the generated multi-view videos.\n10.3.4 Examples\nFigure 27 provides typical examples of videos with good and bad quality in terms of 3D Object Tracking.\n10.3.5 Evaluation & Analysis\nTable 22 provides the complete results of models in terms of 3D Object Tracking.\n.\n10.4 Occupancy Prediction\n10.4.1 Definition\nOccupancy Prediction evaluates whether generated videos enable accurate 3D reconstruction of scene geometry and semantics. We adopt the RayIoU metric [96], which measures semantic and geometric agreement along camera rays rather than voxel overlap. For each ray, RayIoU compares the frontmost occupied voxel in the predicted and ground-truth volumes, requiring both class correctness and depth proximity within a tolerance . This ray-wise formulation avoids the depth-ambiguity of voxel mIoU (which may reward thick surfaces) and naturally supports multi-pose scene completion evaluation via ray casting.\n10.4.2 Formulation\nA frozen occupancy estimator predicts a probabilistic 3D volume for each generated video:\nLet denote the set of sampled query rays (with distance-balanced resampling). For each ray , denote the frontmost occupied voxel in prediction and ground truth by and . A prediction is correct if and . The RayIoU at tolerance is defined as:\nand the mean RayIoU (mRayIoU) aggregates multiple tolerances:\nThe dataset-level semantic occupancy score averages mRayIoU across all generated videos:\nHigher indicates that generated scenes enable more accurate, depth-consistent, and semantically faithful occupancy reconstruction.\n10.4.3 Implementation Details\nWe perform occupancy prediction using the pretrained SparseOcc model [96]. The model is applied to the generated multi-view frames following the official nuScenes camera-only configuration, producing voxel-wise semantic occupancy grids within a 3D volume for evaluation.\n10.4.4 Examples\nFigure 28 provides typical examples of videos with good and bad quality in terms of Occupancy Prediction.\n10.4.5 Evaluation & Analysis\nTable 23 provides the complete results of models in terms of Occupancy Prediction.\n[ADDRESS_REMOVED] 5: Human Preference\nThis section presents human-centered evaluations. While quantitative measures capture specific aspects of fidelity, consistency, and geometric accuracy, they cannot fully reflect how humans perceive realism, stability, and overall scene quality. To bridge this gap, we introduce a human preference study that scores generated videos across multiple dimensions, providing a holistic and perceptually grounded assessment of model performance.\n11.1 World Realism - Overall Realism\nOverall Realism measures the global visual believability. Annotators judge whether the generated video “looks like a real-world driving recording”. They are instructed to judge each clip based on the following criteria:\n-\n•\nStructural and perspective coherence of the environment.\n-\n•\nVisual stability without severe flicker, tearing, or geometric warping.\n-\n•\nRealistic lighting, shadows, and surface textures.\n-\n•\nConsistent composition of static (roads, buildings, sky) and dynamic (vehicles, pedestrians) elements.\nHigher Overall Realism indicates that generated scenes are globally coherent, visually stable, and perceptually indistinguishable from real-world videos.\n11.1.1 Protocol\nEach generated video is rated on a – scale of perceived realism:\n11.1.2 Examples\nFigure 29 provides typical examples of videos with good and bad quality in terms of Overall Realism.\n11.1.3 Evaluation & Analysis\nTable 24 provides the complete results of models in terms of Overall Realism.\n11.2 World Realism - Vehicle Realism\nVehicle Realism isolates the perceptual authenticity of vehicles within the scene, focusing solely on their visual appearance. Annotators evaluate whether vehicles “look like real cars\". Annotators are instructed to judge each clip according to the following criteria:\n-\n•\nCorrect body shape, door/roof/wheel-arch proportions, and stable contours without deformation.\n-\n•\nRealistic metallic paint, plastic, glass, tires, and recognizable small components (logos, grilles, lamps).\n-\n•\nNatural highlights, shadows, and reflections under various weather and illumination conditions.\n-\n•\nColor, texture, and boundary stability across adjacent frames.\nHigh Vehicle Realism reflects consistent car geometry, convincing materials, physically plausible reflections, and temporally stable rendering. Low scores correspond to warped, “rubber-like\" cars with flickering colors, melted textures, or incoherent lighting.\n11.2.1 Protocol\nEach generated video is rated on a – scale of perceived realism:\n11.2.2 Examples\nFigure 30 provides typical examples of videos with good and bad quality in terms of Vehicle Realism.\n11.2.3 Evaluation & Analysis\nTable 25 provides the complete results of models in terms of Vehicle Realism.\n11.3 World Realism - Pedestrian Realism\nPedestrian Realism measures whether humans in generated videos look and move like real people. It focuses on anatomical plausibility, natural appearance, and temporal stability of pedestrians. Annotators are instructed to judge each clip according to the following criteria:\n-\n•\nRealistic head-torso-limb ratios, joint positions, and poses without twisted or intersecting limbs.\n-\n•\nPlausible garment structure, texture clarity, and consistency of accessories.\n-\n•\nSmooth and natural shading without wax-like or distorted faces.\n-\n•\nContinuous appearance without flickering, sliding, or sudden disappearance.\n-\n•\nWhether pedestrians resemble real filmed humans rather than avatars or composites.\nHigher Pedestrian Realism indicates pedestrians with stable body structures, coherent motion, realistic textures, and natural temporal behavior.\n11.3.1 Protocol\nEach generated video is rated on a – scale of perceived realism:\n11.3.2 Examples\nFigure 31 provides typical examples of videos with good and bad quality in terms of Pedestrian Realism.\n11.3.3 Evaluation & Analysis\nTable 26 provides the complete results of models in terms of Pedestrian Realism.\n11.4 Physical Plausibility\nPhysical Plausibility evaluates whether the motions, interactions, and visual evolution of a generated driving scene are consistent with basic physical laws and causal structure in the real world. This dimension explicitly targets physics and dynamics: whether objects move, collide, occlude, and respond in ways that respect continuity, inertia, contact, and depth ordering. Annotators are instructed to judge each clip according to the following criteria:\n-\n•\nPositions, velocities, colors, and textures should evolve smoothly over time, without teleportation, duplication, spontaneous appearance or disappearance, or violent jumps in shape or brightness.\n-\n•\nVehicles, pedestrians, and static elements (barriers, poles, buildings) should not interpenetrate. Feet should visually remain on the ground when walking, and objects should not float or sink into surfaces.\n-\n•\nForeground and background elements should obey consistent occlusion relationships. Distant objects should not suddenly occlude closer ones, and elements like traffic lights, fences, and signboards should not phase through other geometry.\n-\n•\nHighlights, reflections, glare, and shadows should change smoothly with camera motion and object movement, without unexplained flashes, patches of incoherent reflection, or abrupt brightness jumps.\nHigher Physical Plausibility indicates that generated worlds exhibit more physically consistent dynamics.\n11.4.1 Protocol\nEach generated video is rated on a – scale of perceived realism:\n11.4.2 Examples\nFigure 32 provides typical examples of videos with good and bad quality in terms of Physical Plausibility.\n11.4.3 Evaluation & Analysis\nTable 27 provides the complete results of models in terms of Physical Plausibility.\n11.5 3D & 4D Consistency\nPhysical Plausibility measures how well the 3D structure and temporal evolution of objects in a generated video align with those in the corresponding real (ground-truth) sequence. Rather than judging raw pixels, this dimension focuses on the stability and accuracy of 3D bounding boxes over time, as estimated by a pretrained tracking or detection model applied to both generated and real videos. Annotators are instructed to judge each clip according to the following criteria:\n-\n•\nFor each object, the 3D box size, orientation, and position should evolve smoothly over time, without jitter, sudden jumps, unnatural scaling, or misalignment with the underlying object.\n-\n•\nTracks should persist as long as the object is visible, without frequent flickering, disappearing-and-reappearing, or drifting away from the target.\n-\n•\nThe number and spatial arrangement of boxes in the generated view should broadly match those in the ground-truth view, especially for prominent nearby objects.\n-\n•\nIn dynamic scenes, the motion direction and speed of boxes in the generated view should be close to those in the ground-truth view; in static scenes, boxes should remain essentially still.\nHigher scores indicate that 3D box trajectories in generated videos closely track those in real scenes.\n11.5.1 Protocol\nEach generated video is rated on a – scale of perceived realism:\n11.5.2 Examples\nFigure 33 provides typical examples of videos with good and bad quality in terms of 3D & 4D Consistency.\n11.5.3 Evaluation & Analysis\nTable 28 provides the complete results of models in terms of 3D & 4D Consistency.\n11.6 Behavioral Safety\nBehavioral Safety measures how safe and predictable the visible behavior of traffic participants appears in generated driving videos, as judged by human observers. Rather than evaluating visual realism alone, this dimension focuses on whether vehicles, pedestrians, cyclists, and other agents interact with each other and with key scene elements in a way that is consistent with basic traffic rules and low-risk driving. Annotators are instructed to judge each clip according to the following criteria:\n-\n•\nObvious impossible behaviors, e.g., sudden teleportation, splitting or merging of agents, agents appearing or disappearing without cause, or severe shape deformation that destroys basic spatial relations.\n-\n•\nWhether agent behavior clearly contradicts prominent traffic signals, signs, or lane markings (for example, ignoring a red light, driving against traffic, or violating stop or yield indications).\n-\n•\nWhether vehicles and road users maintain reasonable gaps, avoid implausible near-collisions or illegal crossings, and follow trajectories that are smooth and predictable rather than erratic or conflict-prone.\n-\n•\nWhether scene distortions, flickering, or object deformations directly impair the ability to read safety-critical cues, such as lane boundaries, signal states, and relative positions of agents.\nHigher Behavioral Safety indicates that generated videos tend to display traffic behavior that raters judge as safe and consistent with basic road rules.\n11.6.1 Protocol\nEach generated video is rated on a – scale of perceived realism:\n11.6.2 Examples\nFigure 34 provides typical examples of videos with good and bad quality in terms of Behavioral Safety.\n11.6.3 Evaluation & Analysis\nTable 29 provides the complete results of models in terms of Behavioral Safety.\n12 Evaluation Agent\nIn this section, we present additional detail on the proposed WorldLens-Agent model, describing the architecture, prompting scheme, training setup, and providing some qualitative evaluation examples on out-of-distribution driving videos. We observe that evaluating generated worlds often hinges on human-centered criteria (physical plausibility) and subjective preferences (perceived realism) that quantitative metrics inherently miss. Our goal here is to train an auto-evaluation agent that can be utilized in a broader range of generated videos, and, simultaneously, align with the preferences of human annotators.\n12.1 Agent Architecture\nThe WorldLens-Agent is a vision-language critic built on Qwen3-VL-8B [2] and trained to evaluate generated videos along human-centered dimensions, including overall realism, 3D consistency, physical plausibility, and behavioral safety.\nAs shown in Figure 35, the agent takes two types of input: an instruction text describing the evaluation criteria, which is processed by the frozen Qwen3 tokenizer, and a video generated by world models, which is encoded by the frozen Qwen3-VL vision encoder. The resulting features are projected into the language token space, forming a unified multimodal token sequence.\nThis sequence is then passed to the Qwen3-VL decoder, where LoRA adapters are applied only to the attention layers. All other components, including the vision encoder, the projector, the embedding layers, and the MLP blocks, remain frozen. This lightweight adaptation allows the model to incorporate human perceptual and safety-related priors learned from WorldLens-26K, enabling it to capture cues such as lighting realism, depth stability, object dynamics, and safety-critical violations while preserving the general multimodal capability of the base model.\nFinally, the agent autoregressively produces a structured JSON output that contains a numerical score (-) and a concise rationale for each evaluation dimension. This representation yields a reliable, interpretable, and scalable assessment signal that complements conventional quantitative metrics and serves as a consistent preference oracle for world-model benchmarking and downstream reinforcement learning pipelines.\n12.2 Prompt Scheme\nThe following prompting scheme specifies the instruction protocol for the WorldLens Evaluation Agent. Given a generated driving clip and a dimension-specific human rating rubric, the agent is guided to produce structured, evidence-based scores for multiple aspects of generative video quality.\nThe prompt enforces strict output formatting, dimension-aware reasoning, and rubric-consistent interpretation, ensuring reliable and reproducible automatic scoring.\n12.3 Training Setup\nThe WorldLens-Agent is fine-tuned from Qwen3-VL-8B through supervised instruction tuning, allowing the model to better align with human evaluation preferences. We adopt LoRA adaptation on all attention modules, using a rank of and a dropout rate of , which provides efficient preference learning while preserving the multimodal reasoning capabilities of the base model.\nTraining is performed for three epochs with a learning rate of e-, cosine decay scheduling, and a warmup ratio of . All experiments are conducted on eight A100 GPUs using bfloat16 precision. This configuration ensures stable convergence and effective adaptation, resulting in a vision-language critic that consistently captures realism, geometric consistency, physical plausibility, and safety-related cues in generated videos.\n12.4 Qualitative Assessment\nFigure 36 and Figure 37 present additional qualitative evaluations produced by the WorldLens-Agent on challenging driving scenarios, including out-of-distribution videos rendered or produced by Gen3C [83], Cosmos-Drive [82], and the CARLA [23] simulator. These examples illustrate the agent’s ability to generalize beyond its training distribution and maintain consistent, human-aligned judgment across a wide spectrum of visual styles, scene structures, and motion dynamics.\nAs shown in Figure 36, the agent reliably identifies a broad range of safety-critical issues. These include lane incursions, ignoring red lights, and near-collision events, each accompanied by a concise explanation grounded in visible evidence. It also detects failures in physical plausibility, such as unnatural animal motion or vehicles exhibiting incoherent dynamics, where motion lacks realistic articulation or violates expected mass-gravity relationships. In the realm of realism, the agent highlights artifacts like low-fidelity textures, simplified geometry, and game-engine rendering effects, all of which degrade perceptual authenticity.\nFigure 37 further demonstrates the agent’s sensitivity to more severe and uncommon failure modes. It flags physically impossible ego-vehicle trajectories, such as the viewpoint unexpectedly lifting off the ground, as violations of basic mechanical and gravitational constraints. The agent also captures high-impact behavioral safety failures, including colliding with stationary obstacles such as ambulances or trucks. Beyond temporal or behavioral issues, it robustly identifies large-scale 3D and 4D consistency violations, where buildings, vehicles, and road structures visibly intersect or pass through one another, indicating broken geometry and disrupted spatial coherence.\nTogether, these qualitative cases highlight the strong generalization capability of the proposed WorldLens-Agent and our ability to diagnose diverse, complex failure patterns across unseen generative video domains. The agent not only assigns scalar scores but also provides clear, interpretable rationales, enabling transparent and human-aligned evaluation under significant distribution shift.\n[ADDRESS_REMOVED] & Limitations\nIn this section, we elaborate on the broader impact, societal influence, and potential limitations of the proposed approach.\n13.[ADDRESS_REMOVED]\nOur benchmark advances the evaluation of generative world models by establishing a unified, transparent, and reproducible protocol that links perception, geometry, physics, and behavior. By grounding quantitative scores in human perception and physical reasoning, we encourage the development of models that are not only visually convincing but also physically reliable and functionally safe.\nThe benchmark, dataset, and agent together promote standardization and comparability in this rapidly evolving domain, helping researchers diagnose weaknesses, track progress, and design more robust embodied simulators. Beyond autonomous driving, the framework can inspire principled evaluation methods for robotics, AR/VR simulation, and broader world-model research.\n13.2 Societal Influence\nOur benchmark has implications for AI safety, trustworthy simulation, and embodied intelligence. By providing quantitative and human-aligned metrics for realism, physical plausibility, and behavioral safety, our benchmark helps mitigate risks from models that may appear realistic but behave unrealistically when used for planning or training downstream agents. Reliable evaluation of generative simulators could accelerate applications in safe-driving research, synthetic dataset generation, and policy testing under controlled conditions.\nNonetheless, the framework should be used responsibly, especially when synthetic data influence safety-critical decisions, ensuring transparency in evaluation and avoiding misuse for deceptive content generation.\n13.3 Potential Limitations\nWhile our benchmark provides a comprehensive evaluation spectrum, several limitations remain. First, the benchmark currently focuses on driving-world scenarios; extending to indoor, aerial, or humanoid environments requires additional metrics and domain-specific cues. Second, although the human preference dataset (WorldLens-26K) captures rich perceptual reasoning, it may reflect annotator bias toward specific visual styles or regions, which future work could mitigate through more diverse and cross-cultural labeling. Third, the evaluation agent, though effective in zero-shot settings, inherits limitations from its underlying language model and supervision quality. Lastly, physical realism in simulation is inherently open-ended; new metrics may be required as models evolve toward interactive and multimodal 4D reasoning.\n14 Public Resource Used\nIn this section, we acknowledge the use of the following public resources, during the course of this work:\n-\n•\nnuScenes111https://www.nuscenes.org/nuscenes. ........................................................................................................................................................................CC BY-NC-SA 4.0\n-\n•\nnuscenes-devkit222https://github.com/nutonomy/nuscenes-devkit. ........................................................................................................................................................................Apache License 2.0\n-\n•\nKITTI333http://www.cvlibs.net/datasets/kitti. ........................................................................................................................................................................Non-Commercial Use Only (Research Purposes)\n-\n•\nwaymo-open-dataset444https://github.com/waymo-research/waymo-open-dataset. ........................................................................................................................................................................Apache License 2.0\n-\n•\nMagicDrive555https://github.com/cure-lab/MagicDrive. ........................................................................................................................................................................Apache License 2.0\n-\n•\nDreamForge666https://github.com/PJLab-ADG/DriveArena. ........................................................................................................................................................................Apache License 2.0\n-\n•\nDriveDreamer-2777https://github.com/f1yfisher/DriveDreamer2. ........................................................................................................................................................................Apache License 2.0\n-\n•\nOpenDWM888https://github.com/SenseTime-FVG/OpenDWM. ........................................................................................................................................................................MIT License\n-\n•\nDiST-4D999https://github.com/royalmelon0505/dist4d. ........................................................................................................................................................................None\n-\n•\n-Scene101010https://github.com/yuyang-cloud/X-Scene. .None\n-\n•\nPanacea111111https://github.com/wenyuqing/panacea. ........................................................................................................................................................................Apache License 2.0\n-\n•\nLimsim121212https://github.com/PJLab-ADG/LimSim/tree/LimSim_plus. ........................................................................................................................................................................None\n-\n•\nDriveStudio131313https://github.com/ziyc/drivestudio. ........................................................................................................................................................................MIT License\n-\n•\nDriveArena141414https://github.com/PJLab-ADG/DriveArena. ........................................................................................................................................................................None\n-\n•\nDrivingSphere151515https://github.com/yanty123/DrivingSphere. ........................................................................................................................................................................Apache License 2.0\n-\n•\nMagicDrive-V2161616https://github.com/flymin/MagicDrive-V2. ........................................................................................................................................................................AGPL-3.0 license\n-\n•\nUniAD171717https://github.com/OpenDriveLab/UniAD. ........................................................................................................................................................................Apache License 2.0\n-\n•\nOpen3D181818http://www.open3d.org. ........................................................................................................................................................................MIT License\n-\n•\nPyTorch191919https://pytorch.org. ........................................................................................................................................................................BSD License\n-\n•\nROS Humble202020https://docs.ros.org/en/humble. ........................................................................................................................................................................Apache License 2.0\n-\n•\ntorchsparse212121https://github.com/mit-han-lab/torchsparse. ........................................................................................................................................................................MIT License\n-\n•\nVBench222222https://github.com/Vchitect/VBench. ........................................................................................................................................................................Apache License 2.0\n-\n•\nSparseOcc232323https://github.com/MCG-NJU/SparseOcc. ........................................................................................................................................................................Apache License 2.0\n-\n•\nDINO242424https://github.com/facebookresearch/dino. ........................................................................................................................................................................Apache License 2.0\n-\n•\nDINOv2252525https://github.com/facebookresearch/dinov2. ........................................................................................................................................................................Apache License 2.0\n-\n•\nMMEngine262626https://github.com/open-mmlab/mmengine. ........................................................................................................................................................................Apache License 2.0\n-\n•\nMMCV272727https://github.com/open-mmlab/mmcv. ........................................................................................................................................................................Apache License 2.0\n-\n•\nMMDetection282828https://github.com/open-mmlab/mmdetection. ........................................................................................................................................................................Apache License 2.0\n-\n•\nMMDetection3D292929https://github.com/open-mmlab/mmdetection3d. ........................................................................................................................................................................Apache License 2.0\n-\n•\nOpenPCSeg303030https://github.com/PJLab-ADG/OpenPCSeg. ........................................................................................................................................................................Apache License 2.0\n-\n•\nOpenPCDet313131https://github.com/open-mmlab/OpenPCDet. ........................................................................................................................................................................Apache License 2.0\n-\n•\nQwen3-VL323232https://github.com/QwenLM/Qwen3-VL. ........................................................................................................................................................................Apache License 2.0\n-\n•\nLLaMA-Factory333333https://github.com/hiyouga/LLaMA-Factory. ........................................................................................................................................................................Apache License 2.0\nReferences\n- Arai et al. [2024] Hidehisa Arai, Keishi Ishihara, Tsubasa Takahashi, and Yu Yamaguchi. ACT-Bench: Towards action controllable world models for autonomous driving. arXiv preprint arXiv:2412.[POSTAL_CODE_REMOVED], 2024.\n- Bai et al. [2025] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.[POSTAL_CODE_REMOVED], 2025.\n- Ball et al. [2025] Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell, Aäron van den Oord, Inbar Mosseri, Adrian Bolton, Satinder Singh, and Tim Rocktäschel. Genie 3: A new frontier for world models, 2025.\n- Bar-Tal et al. [2024] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: A space-time diffusion model for video generation. In SIGGRAPH Asia, pages 1–11, 2024.\n- Bartoccioni et al. [2025] Florent Bartoccioni, Elias Ramzi, Victor Besnier, Shashanka Venkataramanan, Tuan-Hung Vu, Yihong Xu, Loick Chambon, Spyros Gidaris, Serkan Odabas, David Hurych, Renaud Marlet, Alexandre Boulch, Mickael Chen, Éloi Zablocki, Andrei Bursuc, Eduardo Valle, and Matthieu Cord. VaViM and VaVAM: Autonomous driving through video generative modeling. arXiv preprint arXiv:2502.[POSTAL_CODE_REMOVED], 2025.\n- Betker et al. [2023] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science, 2(3):8, 2023.\n- Bian et al. [2025] Hengwei Bian, Lingdong Kong, Haozhe Xie, Liang Pan, Yu Qiao, and Ziwei Liu. DynamicCity: Large-scale 4D occupancy generation from dynamic scenes. In International Conference on Learning Representations, 2025.\n- Blattmann et al. [2023] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2023.\n- Caesar et al. [2020] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes: A multimodal dataset for autonomous driving. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2020.\n- Caesar et al. [2021] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.[POSTAL_CODE_REMOVED], 2021.\n- Cao et al. [2025] Wei Cao, Marcel Hallgarten, Tianyu Li, Daniel Dauner, Xunjiang Gu, Caojun Wang, Yakov Miron, Marco Aiello, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, and Kashyap Chitta. Pseudo-simulation for autonomous driving. In Conference on Robot Learning. PMLR, 2025.\n- Caron et al. [2021] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In IEEE/CVF International Conference on Computer Vision, pages 9650–9660, 2021.\n- Carreira and Zisserman [2017] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6299–6308, 2017.\n- Chen et al. [2025a] Anthony Chen, Wenzhao Zheng, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Kurt Keutzer, and Shanghang Zhang. GeoDrive: 3D geometry-informed driving world model with precise action control. arXiv preprint arXiv:2505.[POSTAL_CODE_REMOVED], 2025a.\n- Chen et al. [2025b] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025b.\n- Chen et al. [2024] Yuntao Chen, Yuqi Wang, and Zhaoxiang Zhang. DrivingGPT: Unifying driving world modeling and planning with multi-modal autoregressive transformers. arXiv preprint arXiv:2412.[POSTAL_CODE_REMOVED], 2024.\n- Chen et al. [2025c] Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, and Yue Wang. OmniRe: Omni urban scene reconstruction. In International Conference on Learning Representations, 2025c.\n- Cui et al. [2025] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Self-forcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.[POSTAL_CODE_REMOVED], 2025.\n- Dalal and Triggs [2005] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 886–893, 2005.\n- Dauner et al. [2024] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, and Kashyap Chitta. NAVSIM: Data-driven non-reactive autonomous vehicle simulation and benchmarking. In Advances in Neural Information Processing Systems, volume 37, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2024.\n- Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009.\n- Ding et al. [2024] Shuxiao Ding, Lukas Schneider, Marius Cordts, and Juergen Gall. ADA-Track: End-to-end multi-camera 3D multi-object tracking with alternating detection and association. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2024.\n- Dosovitskiy et al. [2017] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In Conference on Robot Learning, pages 1–16. PMLR, 2017.\n- Dosovitskiy et al. [2021] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\n- Duan et al. [2025] Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. WorldScore: A unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.[POSTAL_CODE_REMOVED], 2025.\n- Fan et al. [2025] Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, et al. Vchitect-2.0: Parallel transformer for scaling up video diffusion models. arXiv preprint arXiv:2501.[POSTAL_CODE_REMOVED], 2025.\n- Gao et al. [2023] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. MagicDrive: Street view generation with diverse 3D geometry control. In International Conference on Learning Representations, 2023.\n- Gao et al. [2025] Ruiyuan Gao, Kai Chen, Bo Xiao, Lanqing Hong, Zhenguo Li, and Qiang Xu. MagicDrive-V2: High-resolution long video generation for autonomous driving with adaptive control. In IEEE/CVF International Conference on Computer Vision, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025.\n- Gao et al. [2024] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: A generalizable driving world model with high fidelity and versatile controllability. In Advances in Neural Information Processing Systems, volume 37, 2024.\n- Guo et al. [2025a] Jiazhe Guo, Yikang Ding, Xiwu Chen, Shuo Chen, Bohan Li, Yingshuang Zou, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Zhiheng Li, and Hao Zhao. DiST-4D: Disentangled spatiotemporal diffusion with metric depth for 4D driving scene generation. In IEEE/CVF International Conference on Computer Vision, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025a.\n- Guo et al. [2025b] Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, and Jiang Bian. MineWorld: A real-time and open-source interactive world model on MineCraft. arXiv preprint arXiv:2504.[POSTAL_CODE_REMOVED], 2025b.\n- Hassan et al. [2025] Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Pedro M B Rezende, Yasaman Haghighi, David Brüggemann, Isinsu Katircioglu, Lin Zhang, Xiaoran Chen, Suman Saha, Marco Cannici, Elie Aljalbout, Botao Ye, Xi Wang, Aram Davtyan, Mathieu Salzmann, Davide Scaramuzza, Marc Pollefeys, Paolo Favaro, and Alexandre Alahi. GEM: A generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025.\n- He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\n- He et al. [2021] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, and Wei Jiang. TransReID: Transformer-based object re-identification. In IEEE/CVF international conference on computer vision, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2021.\n- Heusel et al. [2017] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two-time-scale update rule converge to a local Nash equilibrium. Advances in Neural Information Processing Systems, 30:6629–6640, 2017.\n- Ho et al. [2022] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.[POSTAL_CODE_REMOVED], 2022.\n- Hu et al. [2023a] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. GAIA-1: A generative world model for autonomous driving. arXiv preprint arXiv:2309.[POSTAL_CODE_REMOVED], 2023a.\n- Hu et al. [2023b] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2023b.\n- Huang et al. [2025a] Binyuan Huang, Yuqing Wen, Yucheng Zhao, Yaosi Hu, Yingfei Liu, Fan Jia, Weixin Mao, Tiancai Wang, Chi Zhang, Chang Wen Chen, Zhenzhong Chen, and Xiangyu Zhang. SubjectDrive: Scaling generative data in autonomous driving via subject control. In AAAI Conference on Artificial Intelligence, volume 39, pages 3617–3625, 2025a.\n- Huang et al. [2025b] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.[POSTAL_CODE_REMOVED], 2025b.\n- Huang et al. [2024a] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2024a.\n- Huang et al. [2024b] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. VBench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.[POSTAL_CODE_REMOVED], 2024b.\n- Huang et al. [2025c] Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu, Paul Debevec, and Ziwei Liu. VChain: Chain-of-visual-thought for reasoning in video generation. arXiv preprint arXiv:2510.[POSTAL_CODE_REMOVED], 2025c.\n- Jia et al. [2023] Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Yuqing Wen, Chi Zhang, Xiangyu Zhang, and Tiancai Wang. ADriver-I: A general world model for autonomous driving. arXiv preprint arXiv:2311.[POSTAL_CODE_REMOVED], 2023.\n- Jiang et al. [2023] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. VAD: Vectorized scene representation for efficient autonomous driving. In IEEE/CVF International Conference on Computer Vision, pages 8340–8350, 2023.\n- Jiang et al. [2024] Junpeng Jiang, Gangyi Hong, Lijun Zhou, Enhui Ma, Hengtong Hu, Xia Zhou, Jie Xiang, Fan Liu, Kaicheng Yu, Haiyang Sun, Kun Zhan, Peng Jia, and Miao Zhang. DiVE: DiT-based video generation with enhanced control. arXiv preprint arXiv:2409.[POSTAL_CODE_REMOVED], 2024.\n- Kang et al. [2024] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: A physical law perspective. arXiv preprint arXiv:2411.[POSTAL_CODE_REMOVED], 2024.\n- Kay et al. [2017] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The Kinetics human action video dataset. arXiv preprint arXiv:1705.[POSTAL_CODE_REMOVED], 2017.\n- Ke et al. [2021] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. MUSIQ: Multi-scale image quality transformer. In IEEE/CVF International Conference on Computer Vision, pages 5148–5157, 2021.\n- Kerbl et al. [2023] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3D Gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1–14, 2023.\n- Kodaira et al. [2025] Akio Kodaira, Chenfeng Xu, Toshiki Hazama, Takanori Yoshimoto, Kohei Ohno, Shogo Mitsuhori, Soichi Sugano, Hanying Cho, Zhijian Liu, Masayoshi Tomizuka, et al. StreamDiffusion: A pipeline-level solution for real-time interactive generation. In IEEE/CVF International Conference on Computer Vision, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025.\n- Kong et al. [2024] Lingdong Kong, Shaoyuan Xie, Hanjiang Hu, Yaru Niu, Wei Tsang Ooi, Benoit R. Cottereau, Lai Xing Ng, Yuexin Ma, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu, Weichao Qiu, Wei Zhang, Xu Cao, Hao Lu, Ying-Cong Chen, et al. The RoboDrive challenge: Drive anytime anywhere in any condition. arXiv preprint arXiv:2405.[POSTAL_CODE_REMOVED], 2024.\n- Kong et al. [2025] Lingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, Junyuan Deng, Kaiwen Zhang, Yang Wu, Tianyi Yan, Shenyuan Gao, Song Wang, Linfeng Li, Liang Pan, Yong Liu, Jianke Zhu, Wei Tsang Ooi, Steven C. H. Hoi, and Ziwei Liu. 3D and 4D world modeling: A survey. arXiv preprint arXiv:2509.[POSTAL_CODE_REMOVED], 2025.\n- Lab [2024] Pika Lab. Pika. [URL_REMOVED] 2024.\n- Li et al. [2024a] Chengxuan Li, Di Huang, Zeyu Lu, Yang Xiao, Qingqi Pei, and Lei Bai. A survey on long video generation: Challenges, methods, and prospects. arXiv preprint arXiv:2403.[POSTAL_CODE_REMOVED], 2024a.\n- Li et al. [2024b] Xiaofan Li, Yifu Zhang, and Xiaoqing Ye. DrivingDiffusion: Layout-guided multi-view driving scenarios video generation with latent diffusion model. In European Conference on Computer Vision, pages 469–485. Springer, 2024b.\n- Li et al. [2025a] Zhikai Li, Xuewen Liu, Dongrong Joe Fu, Jianquan Li, Qingyi Gu, Kurt Keutzer, and Zhen Dong. K-sort arena: Efficient and reliable benchmarking for generative models via k-wise human preferences. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9131–9141, 2025a.\n- Li et al. [2025b] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. BEVFormer: Learning bird’s-eye-view representation from LiDAR-camera via spatiotemporal transformers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 47(3):2020–2036, 2025b.\n- Liang et al. [2025] Ao Liang, Lingdong Kong, Dongyue Lu, Youquan Liu, Jian Fang, Huaici Zhao, and Wei Tsang Ooi. Perspective-invariant 3D object detection. In IEEE/CVF International Conference on Computer Vision, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025.\n- Liang et al. [2026] Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, and Wei Tsang Ooi. LiDARCrafter: Dynamic 4D world modeling from LiDAR sequences. In AAAI Conference on Artificial Intelligence, volume 40, 2026.\n- Liao et al. [2024] Mingxiang Liao, Qixiang Ye, Wangmeng Zuo, Fang Wan, Tianyu Wang, Yuzhong Zhao, Jingdong Wang, and Xinyu Zhang. Evaluation of text-to-video generation models: A dynamics perspective. In Advances in Neural Information Processing Systems, volume 37, pages 109790–109816, 2024.\n- Lin et al. [2025] Hongbin Lin, Zilu Guo, Yifan Zhang, Shuaicheng Niu, Yafeng Li, Ruimao Zhang, Shuguang Cui, and Zhen Li. DriveGen: Generalized and robust 3D detection in driving via controllable text-to-image diffusion generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025.\n- Liu et al. [2024a] Xiao Liu, Xinhao Xiang, Zizhong Li, Yongheng Wang, Zhuoheng Li, Zhuosheng Liu, Weidi Zhang, Weiqi Ye, and Jiawei Zhang. A survey of AI-generated video evaluation. arXiv preprint arXiv:2410.[POSTAL_CODE_REMOVED], 2024a.\n- Liu et al. [2024b] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. EvalCrafter: Benchmarking and evaluating large video generation models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2024b.\n- Liu et al. [2026] Youquan Liu, Lingdong Kong, Weidong Yang, Xin Li, Ao Liang, Runnan Chen, Ben Fei, and Tongliang Liu. La La LiDAR: Large-scale layout generation from LiDAR data. In AAAI Conference on Artificial Intelligence, volume 40, 2026.\n- Liu et al. [2023] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L. Rus, and Song Han. BEVFusion: Multi-task multi-sensor fusion with unified bird’s-eye view representation. In IEEE International Conference on Robotics and Automation, pages 2774–2781, 2023.\n- Lu et al. [2024] Jiachen Lu, Ze Huang, Zeyu Yang, Jiahui Zhang, and Li Zhang. WoVoGen: World volume-aware diffusion for controllable multi-camera driving scene generation. In European Conference on Computer Vision, pages 329–345. Springer, 2024.\n- Ma et al. [2025] Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, Zeyu Wang, Zhifeng Li, Xiu Li, Wei Liu, Dan Xu, Linfeng Zhang, and Qifeng Chen. Controllable video generation: A survey. arXiv preprint arXiv:2507.[POSTAL_CODE_REMOVED], 2025.\n- Mei et al. [2024] Jianbiao Mei, Tao Hu, Xuemeng Yang, Licheng Wen, Yu Yang, Tiantian Wei, Yukai Ma, Min Dou, Botian Shi, and Yong Liu. DreamForge: Motion-aware autoregressive video generation for multi-view driving scenes. arXiv preprint arXiv:2409.[POSTAL_CODE_REMOVED], 2024.\n- Mei et al. [2025] Jianbiao Mei, Yu Yang, Xuemeng Yang, Licheng Wen, Jiajun Lv, Botian Shi, and Yong Liu. Vision-centric 4d occupancy forecasting and planning via implicit residual world models. arXiv preprint arXiv:2510.[POSTAL_CODE_REMOVED], 2025.\n- Mildenhall et al. [2021] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99–106, 2021.\n- Ni et al. [2025] Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, and Zehuan Wu. MaskGWM: A generalizable driving world model with video mask reconstruction. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025.\n- On-Road Automated Driving Committee(2021) [ORAD] On-Road Automated Driving (ORAD) Committee. Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles. [URL_REMOVED] 2021.\n- OpenAI [2024] OpenAI. Sora. Accessed February 15, 2024 [Online] [URL_REMOVED] 2024. URL [URL_REMOVED]\n- Oquab et al. [2024] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal, 2024.\n- Overett et al. [2008] Gary Overett, Lars Petersson, Nathan Brewer, Lars Andersson, and Niklas Pettersson. A new pedestrian dataset for supervised learning. In IEEE Intelligent Vehicles Symposium, pages 373–378, 2008.\n- Parker-Holder et al. [2024] Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktäschel. Genie 2: A large-scale foundation world model, 2024. URL [URL_REMOVED]\n- Peper et al. [2025] Jordan Peper, Zhenjiang Mao, Yuang Geng, Siyuan Pan, and Ivan Ruchkin. Four principles for physically interpretable world models. arXiv preprint arXiv:2503.[POSTAL_CODE_REMOVED], 2025.\n- Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748–8763. PMLR, 2021.\n- Ravi et al. [2025] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. In International Conference on Learning Representations, 2025.\n- Ren et al. [2024] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded SAM: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.[POSTAL_CODE_REMOVED], 2024.\n- Ren et al. [2025a] Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao, Shengyu Huang, Amirmojtaba Sabour, Tianchang Shen, Tobias Pfaff, Jay Zhangjie Wu, Runjian Chen, Seung Wook Kim, Jun Gao, Laura Leal-Taixe, Mike Chen, Sanja Fidler, and Huan Ling. Cosmos-Drive-Dreams: Scalable synthetic driving data generation with world foundation models. arXiv preprint arXiv:2506.[POSTAL_CODE_REMOVED], 2025a.\n- Ren et al. [2025b] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3C: 3D-informed world-consistent video generation with precise camera control. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6121–6132, 2025b.\n- Ren et al. [2025c] Zhongwei Ren, Yunchao Wei, Xun Guo, Yao Zhao, Bingyi Kang, Jiashi Feng, and Xiaojie Jin. VideoWorld: Exploring knowledge learning from unlabeled videos. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025c.\n- Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2022.\n- Russell et al. [2025] Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and Gianluca Corrado. GAIA-2: A controllable multi-view generative world model for autonomous driving. arXiv preprint arXiv:2503.[POSTAL_CODE_REMOVED], 2025.\n- Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:[POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2022.\n- Salimans et al. [2016] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. Advances in Neural Information Processing Systems, 29:2234–2242, 2016.\n- SenseTime-FVG [2025] SenseTime-FVG. Open Driving World Models (OpenDWM). [URL_REMOVED] 2025.\n- Si et al. [2025] Chenyang Si, Weichen Fan, Zhengyao Lv, Ziqi Huang, Yu Qiao, and Ziwei Liu. RepVideo: Rethinking cross-layer representation for video generation. arXiv preprint arXiv:2501.[POSTAL_CODE_REMOVED], 2025.\n- Singer et al. [2024] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6572–6582, 2024.\n- Sun et al. [2021] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8922–8931, 2021.\n- Sun et al. [2025] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2V-CompBench: A comprehensive benchmark for compositional text-to-video generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8406–8416, 2025.\n- Sun et al. [2019] Lei Sun, Kaiwei Wang, Kailun Yang, and Kaite Xiang. See clearer at night: towards robust nighttime semantic segmentation through day-night image conversion. In Artificial Intelligence and Machine Learning in Defense Applications, volume [POSTAL_CODE_REMOVED], pages 77–89. SPIE, 2019.\n- Szegedy et al. [2015] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1–9, 2015.\n- Tang et al. [2024] Pin Tang, Zhongdao Wang, Guoqing Wang, Jilai Zheng, Xiangxuan Ren, Bailan Feng, and Chao Ma. SparseOCC: Rethinking sparse latent representation for vision-based semantic occupancy prediction. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2024.\n- Team [2025a] Google Team. Veo2. Accessed December 18, 2024 [Online] [URL_REMOVED] 2025a. URL [URL_REMOVED]\n- Team [2024a] Kuaishou Team. Kling. Accessed December 9, 2024 [Online] [URL_REMOVED] 2024a. URL [URL_REMOVED]\n- Team [2024b] Tecent Team. HunyuanVideo: A systematic framework for large video generative models, 2024b.\n- Team [2025b] Wan Team. Wan: Open and advanced large-scale video generative models, 2025b.\n- Unterthiner et al. [2018] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.[POSTAL_CODE_REMOVED], 2018.\n- Wang et al. [2025a] Jiarui Wang, Juntong Wang, Xiaorong Zhu, Huiyu Duan, Guangtao Zhai, and Xiongkuo Min. AIGVQA: A unified framework for multi-dimensional quality assessment of AI-generated video. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–3390, 2025a.\n- Wang and Peng [2025] Xiaodong Wang and Peixi Peng. ProphetDWM: A driving world model for rolling out future actions and videos. arXiv preprint arXiv:2505.[POSTAL_CODE_REMOVED], 2025.\n- Wang et al. [2025b] Xiaodong Wang, Zhirong Wu, and Peixi Peng. LongDWM: Cross-granularity distillation for building a long-term driving world model. arXiv preprint arXiv:2506.[POSTAL_CODE_REMOVED], 2025b.\n- Wang et al. [2024a] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. DriveDreamer: Towards real-world-drive world models for autonomous driving. In European Conference on Computer Vision, pages 55–72. Springer, 2024a.\n- Wang et al. [2025c] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, and Ziwei Liu. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, 133(5):3059–3078, 2025c.\n- Wang et al. [2025d] Yuping Wang, Shuo Xing, Cui Can, Renjie Li, Hongyuan Hua, Kexin Tian, Zhaobin Mo, Xiangbo Gao, Keshu Wu, Sulong Zhou, Hengxu You, Juntong Peng, Junge Zhang, Zehao Wang, Rui Song, Mingxuan Yan, Walter Zimmer, Xingcheng Zhou, Peiran Li, Zhaohan Lu, Chia-Ju Chen, Yue Huang, Ryan A. Rossi, Lichao Sun, Hongkai Yu, Zhiwen Fan, Frank Hao Yang, Yuhao Kang, Ross Greer, Chenxi Liu, Eun Hak Lee, Xuan Di, Xinyue Ye, Liu Ren, Alois Knoll, Xiaopeng Li, Shuiwang Ji, Masayoshi Tomizuka, Marco Pavone, Tianbao Yang, Jing Du, Ming-Hsuan Yang, Hua Wei, Ziran Wang, Yang Zhou, Jiachen Li, and Zhengzhong Tu. Generative AI for autonomous driving: Frontiers and opportunities. arXiv preprint arXiv:2505.[POSTAL_CODE_REMOVED], 2025d.\n- Wang et al. [2024b] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2024b.\n- Wang et al. [2004] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, 2004.\n- Wei et al. [2018] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Person transfer gan to bridge domain gap for person re-identification. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79–88, 2018.\n- Wen et al. [2023] Licheng Wen, Daocheng Fu, Song Mao, Pinlong Cai, Min Dou, Yikang Li, and Yu Qiao. LimSim: A long-term interactive multi-scenario traffic simulator. In IEEE International Conference on Intelligent Transportation Systems, pages 1255–1262, 2023.\n- Wen et al. [2024] Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6902–6912, 2024.\n- Wu et al. [2020] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing for computer vision. arXiv preprint arXiv:2006.[POSTAL_CODE_REMOVED], 2020.\n- Wu et al. [2023] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In IEEE/CVF International Conference on Computer Vision, pages 7623–7633, 2023.\n- Wu et al. [2025a] Wei Wu, Xi Guo, Weixuan Tang, Tingxuan Huang, Chiyu Wang, and Chenjing Ding. DriveScape: Towards high-resolution controllable multi-view driving video generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025a.\n- Wu et al. [2025b] Yanhao Wu, Haoyang Zhang, Tianwei Lin, Lichao Huang, Shujie Luo, Rui Wu, Congpei Qiu, Wei Ke, and Tong Zhang. Generating multimodal driving scenes via next-scene prediction. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6844–6853, 2025b.\n- Xie et al. [2021] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. SegFormer: Simple and efficient design for semantic segmentation with transformers. In Advances in Neural Information Processing Systems, volume 34, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2021.\n- Xie et al. [2025] Shaoyuan Xie, Lingdong Kong, Yuhao Dong, Chonghao Sima, Wenwei Zhang, Qi Alfred Chen, Ziwei Liu, and Liang Pan. Are VLMs ready for autonomous driving? an empirical study from the reliability, data, and metric perspectives. In IEEE/CVF International Conference on Computer Vision, pages 6585–6597, 2025.\n- Xue et al. [2025] Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, and Fei Richard Yu. Human motion video generation: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 47(11):[POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025.\n- Yan et al. [2025a] Tianyi Yan, Wencheng Han, Xia Zhou, Xueyang Zhang, Kun Zhan, Cheng-Zhong Xu, and Jianbing Shen. RLGF: Reinforcement learning with geometric feedback for autonomous driving video generation. In Advances in Neural Information Processing Systems, volume 38, 2025a.\n- Yan et al. [2025b] Tianyi Yan, Dongming Wu, Wencheng Han, Junpeng Jiang, Xia Zhou, Kun Zhan, Cheng zhong Xu, and Jianbing Shen. DrivingSphere: Building a high-fidelity 4D world for closed-loop simulation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025b.\n- Yan et al. [2021] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. VideoGPT: Video generation using VQ-VAE and transformers. arXiv preprint arXiv:2104.[POSTAL_CODE_REMOVED], 2021.\n- Yan et al. [2024] Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, and Sida Peng. Street gaussians: Modeling dynamic urban scenes with gaussian splatting. In European Conference on Computer Vision, pages 156–173. Springer, 2024.\n- Yang et al. [2024a] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, Jun Zhang, Andreas Geiger, Yu Qiao, and Hongyang Li. Generalized predictive model for autonomous driving. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2024a.\n- Yang et al. [2023] Kairui Yang, Enhui Ma, Jibin Peng, Qing Guo, Di Lin, and Kaicheng Yu. BEVControl: Accurately controlling street-view elements with multi-perspective consistency via BEV sketch layout. arXiv preprint arXiv:2308.[POSTAL_CODE_REMOVED], 2023.\n- Yang et al. [2024b] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In Advances in Neural Information Processing Systems, volume 37, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2024b.\n- Yang et al. [2025a] Xuemeng Yang, Licheng Wen, Yukai Ma, Jianbiao Mei, Xin Li, Tiantian Wei, Wenjie Lei, Daocheng Fu, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yong Liu, and Yu Qiao. DriveArena: A closed-loop generative simulation platform for autonomous driving. In IEEE/CVF International Conference on Computer Vision, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025a.\n- Yang et al. [2025b] Yu Yang, Alan Liang, Jianbiao Mei, Yukai Ma, Yong Liu, and Gim Hee Lee. X-Scene: Large-scale driving scene generation with high fidelity and flexible controllability. In Advances in Neural Information Processing Systems, volume 38, 2025b.\n- Yang et al. [2025c] Yu Yang, Jianbiao Mei, Yukai Ma, Siliang Du, Wenqing Chen, Yijie Qian, Yuxiang Feng, and Yong Liu. Driving in the occupancy world: Vision-centric 4d occupancy forecasting and planning via world models for autonomous driving. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 9327–9335, 2025c.\n- Yang et al. [2025d] Zhao Yang, Zezhong Qian, Xiaofan Li, Weixiang Xu, Gongpeng Zhao, Ruohong Yu, Lingsi Zhu, and Longjun Liu. DualDiff+: Dual-branch diffusion for high-fidelity video generation with reward guidance. arXiv preprint arXiv:2503.[POSTAL_CODE_REMOVED], 2025d.\n- Yang et al. [2024c] Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, and Wei Wu. Physical informed driving world model. arXiv preprint arXiv:2412.[POSTAL_CODE_REMOVED], 2024c.\n- Yang et al. [2024d] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. CogVideoX: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.[POSTAL_CODE_REMOVED], 2024d.\n- Yang et al. [2024e] Ziyi Yang, Zaibin Zhang, Zirui Zheng, Yuxian Jiang, Ziyue Gan, Zhiyu Wang, Zijian Ling, Jinsong Chen, Martz Ma, Bowen Dong, Prateek Gupta, Shuyue Hu, Zhenfei Yin, Guohao Li, Xu Jia, Lijun Wang, Bernard Ghanem, Huchuan Lu, Chaochao Lu, Wanli Ouyang, Yu Qiao, Philip Torr, and Jing Shao. Oasis: Open agent social interaction simulations with one million agents. arXiv preprint arXiv:2411.[POSTAL_CODE_REMOVED], 2024e.\n- Ye et al. [2025] Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, and Angjoo Kanazawa. GSplat: An open-source library for Gaussian splatting. Journal of Machine Learning Research, 26(34):1–17, 2025.\n- Yu et al. [2025] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T Freeman, and Jiajun Wu. WonderWorld: Interactive 3D scene generation from a single image. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5916–5926, 2025.\n- Yue et al. [2025] Jingtong Yue, Ziqi Huang, Zhaoxi Chen, Xintao Wang, Pengfei Wan, and Ziwei Liu. Simulating the world model with artificial intelligence: A roadmap. arXiv preprint arXiv:2511.[POSTAL_CODE_REMOVED], 2025.\n- Zhang et al. [2024] Fan Zhang, Shulin Tian, Ziqi Huang, Yu Qiao, and Ziwei Liu. Evaluation agent: Efficient and promptable evaluation framework for visual generative models. In Annual Meeting of the Association for Computational Linguistics, 2024.\n- Zhang et al. [2023] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianwei Yang, and Lei Zhang. A simple framework for open-vocabulary segmentation and detection. In IEEE/CVF International Conference on Computer Vision, pages 1020–1031, 2023.\n- Zhang et al. [2025a] Kaiwen Zhang, Zhenyu Tang, Xiaotao Hu, Xingang Pan, Xiaoyang Guo, Yuan Liu, Jingwei Huang, Li Yuan, Qian Zhang, Xiao-Xiao Long, Xun Cao, and Wei Yin. Epona: Autoregressive diffusion world model for autonomous driving. In IEEE/CVF International Conference on Computer Vision, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025a.\n- Zhang et al. [2018] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 586–595, 2018.\n- Zhang et al. [2025b] Zhichao Zhang, Wei Sun, and Guangtao Zhai. A perspective on quality evaluation for AI-generated videos. Sensors, 25:4668, 2025b.\n- Zhao et al. [2025] Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, and Xingang Wang. DriveDreamer-2: LLM-enhanced world models for diverse driving video generation. In AAAI Conference on Artificial Intelligence, volume 39, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025.\n- Zheng et al. [2025] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Lulu Gu, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. VBench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.[POSTAL_CODE_REMOVED], 2025.\n- Zheng et al. [2015] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification: A benchmark. In IEEE/CVF International Conference on Computer Vision, pages 1116–1124, 2015.\n- Zhou et al. [2025] Xin Zhou, Dingkang Liang, Sifan Tu, Xiwu Chen, Yikang Ding, Dingyuan Zhang, Feiyang Tan, Hengshuang Zhao, and Xiang Bai. HERMES: A unified self-driving world model for simultaneous 3D scene understanding and generation. In IEEE/CVF International Conference on Computer Vision, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2025.\n- Zhou et al. [2024] Yunsong Zhou, Michael Simon, Zhenghao Mark Peng, Sicheng Mo, Hongzi Zhu, Minyi Guo, and Bolei Zhou. SimGen: Simulator-conditioned driving scene generation. In Advances in Neural Information Processing Systems, volume 37, pages [POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2024.\n- Zhu et al. [2025] Dekai Zhu, Yixuan Hu, Youquan Liu, Dongyue Lu, Lingdong Kong, and Slobodan Ilic. SPIRAL: Semantic-aware progressive LiDAR scene generation. In Advances in Neural Information Processing Systems, volume 38, 2025.\n- Zuo et al. [2024] Jialong Zuo, Ying Nie, Hanyu Zhou, Huaxin Zhang, Haoyu Wang, Tianyu Guo, Nong Sang, and Changxin Gao. Cross-video identity correlating for person re-identification pre-training. Advances in Neural Information Processing Systems, 37:[POSTAL_CODE_REMOVED]–[POSTAL_CODE_REMOVED], 2024."
},
{
  "article": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion\nand Pose Estimation Model\nAbstract\nWe propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at [URL_REMOVED]\n1 Introduction\nOpen-set 3D scene generation aims to synthesize 3D scenes containing arbitrary objects in any open-world domain from a single image. It is a fundamental task with high demand in AIGC and embodied AI, including applications such as 3D asset creation, simulation environment construction, and 3D perception for decision-making. However, limited scene datasets [fu20213dfront, dai2017scannet, azinovic2022blenderswap] have confined most existing methods [tang2024diffuscene, dahnert2024coherent, liu2022instpifu, dai2024acdc, nie2020total3dunderstanding, dahnert2021panoptic, chen2024ssr, gao2024diffcad] to constrained domains like indoor scenes.\nRecently, the advent of large-scale 3D object datasets [deitke2023objaverse] has driven rapid progress in open-set 3D object generation models [zhang2024clay, wu2024direct3d, li2024craftsman, li2025triposg, xiang2025trellis, zhao2025hunyuan3d, li2025step1x], and emerging methods [yao2025cast, huang2024midi, lin2025partcrafter, meng2025scenegen, ardelean2024gen3dsr] are beginning to extend scene generation toward open-set settings. Despite all the progress, existing methods still struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings in Figure 10.\nThe root cause is the model’s insufficient open-set priors for de-occlusion and pose estimation. As illustrated in Figure 2, a 3D scene generation model requires three key open-set priors in columns: de-occlusion, object geometry, and pose estimation. The availability of these priors varies across scene, object, and image datasets in rows [fu20213dfront, azinovic2022blenderswap, deitke2023objaverse, laion, deng2009imagenet]. Paths in different colors represent various scene generation methods with different prior sources. Existing scene-native methods (yellow path) [tang2024diffuscene, dahnert2024coherent, liu2022instpifu, dai2024acdc] attempt to learn all the three priors exclusively from scene datasets, where the availability of open-set priors is limited. Object-native methods (green path) [yao2025cast, huang2024midi, lin2025partcrafter, meng2025scenegen, ardelean2024gen3dsr, qu2025deocc, wu2025amodal3r] further leverage large-scale 3D object datasets to learn sufficient open-set object geometry priors. However, the open-set priors for de-occlusion and pose estimation still remain insufficient due to the limited datasets, leaving these challenges unresolved. Meanwhile, existing pose estimation methods [wen2024foundationpose, zhang2023genpose, zhang2024omni6dpose] suffer from performance degradation in scene generation task, primarily due to missing size prediction and the absence of tailored attention mechanisms for different pose variables.\nIn this paper, we further advance 3D scene generation towards open-set scenarios by addressing the critical issue of insufficient de-occlusion and pose estimation priors, as shown in Figure 2 (red path). Specifically, we construct a decoupled framework that divides 3D scene generation into three distinct tasks based on the necessary priors: de-occlusion, 3D object generation, and pose estimation. Each task is trained separately on image datasets, 3D object datasets, and scene datasets, respectively. The decoupled framework ensures that each task can maximize the learning of its corresponding open-set priors, preventing quality degradation caused by the cross-impact of data on tasks, such as geometry collapse of small objects and pose shifting resulting from the joint representation of geometry and pose as shown in Figure. 10.\nSecond, we develop a robust de-occlusion model by leveraging image datasets for open-set occlusion prior. Image datasets are significantly larger than 3D datasets, encompassing a broader range of open-set objects and exhibiting more diverse occlusion patterns. To maintain the sufficient open-set priors, we adopt the image editing model [labs2025fluxkontext] as the initialization. Then, we finetune it on our 10K image de-occlusion dataset with three carefully designed occlusion patterns to further enhance its de-occlusion capability, resulting in the final de-occlusion model. Compared with existing 3D object-based methods [wu2025amodal3r, huang2024midi], our model achieves higher quality and more text-controllable results under severe occlusion and open-set conditions.\nThird, we propose a unified pose estimation model along with a 200K scene dataset for better performance and open-set generalization. Since 3D object generation models [zhang2024clay, zhao2025hunyuan3d, li2024craftsman, Chen_2025_Dora] usually output normalized objects in canonical space for better geometry, existing methods [wen2024foundationpose, zhang2023genpose, zhang2024omni6dpose, brazil2023omni3d] often restrict to predefined classes or miss size prediction when they are employed in scene generation task. Thus, we propose a unified diffusion-based pose estimation model, which directly predicts object rotation, translation, and size conditioned on point clouds, images, and object geometry. Compared to existing methods [yao2025cast], we introduce both single object and multi-object self-attention mechanism to ensure interactions between objects for coherent relationships. Moreover, we design a decoupled cross-attention mechanism, where rotation attends to canonical object conditions, while translation and scale attend to scene-level conditions, to further improve accuracy. Additionally, to extend open-set capability, we construct a large-scale synthetic dataset of 200K scenes using Objaverse [deitke2023objaverse] objects and mix it with existing scene datasets during training.\nFinally, comprehensive experiments demonstrate that our model achieves state-of-the-art performance in both object geometry quality and pose accuracy on both indoor and open-set test sets. We further discuss the method’s generalization to varying numbers of objects and its potential upper bound under video and multi-view modalities.\nIn summary, our contributions are as threefold:\n-\n•\nWe construct a decoupled 3D scene generation framework called SceneMaker that fully exploits existing datasets to learn sufficient open-set priors for de-occlusion and pose estimation, achieving superior performance in comprehensive experiments.\n-\n•\nWe develop a robust de-occlusion model by leveraging image datasets for open-set occlusion priors and enhancing it with our 10K object image de-occlusion dataset.\n-\n•\nWe propose a unified pose estimation diffusion model that directly predicts each object’s 6D pose and size, introducing both local and global attention mechanisms to enhance accuracy. And we further curate a 200K synthesized scene dataset for open-set generalization.\n2 Related Work\n2.1 3D Scene Generation\n3D scene generation is in high demand for AIGC and embodied AI, serving as a foundation task for real-to-sim applications. Based on the source of 3D objects, existing methods fall into two categories: generation-based and retrieval-based. Retrieval-based methods [dai2024acdc] retrieve 3D objects from offline libraries but struggle to generalize to open-set scenarios due to limited asset diversity. Generation-based methods directly generate 3D objects from images and can be categorized into scene-native and object-native methods. Scene-native methods [tang2024diffuscene, dahnert2024coherent, liu2022instpifu] directly learn from scene datasets [fu20213dfront, azinovic2022blenderswap, dai2017scannet] but are limited to specific domains like indoor scenes. Object-native methods further leverage open-set 3D object datasets [deitke2023objaverse] to improve object geometry quality. A series of methods [yao2025cast, huang2024midi, lin2025partcrafter, meng2025scenegen, ardelean2024gen3dsr] directly generate object geometry in the scene space. However, due to the limitations of scene datasets and the coupled representation, they often suffer from obvious degradation on images with severe occlusion or small objects. Another series of methods [yao2025cast] decouple geometry generation and pose estimation to improve the open-set performance. But they lack scene-level interactions during pose estimation, leading to inaccurate relative poses. Fundamentally, existing methods lack sufficient de-occlusion and pose estimation priors. We supplement both open-set priors by leveraging image datasets for de-occlusion and proposing a unified model along with synthetic scene datasets for pose estimation.\n2.[ADDRESS_REMOVED] Generation under Occlusion\nWith the emergence of large-scale open-set 3D object datasets [deitke2023objaverse], a number of native 3D object generation works [zhang2024clay, wu2024direct3d, li2024craftsman, li2025triposg, xiang2025trellis, zhao2025hunyuan3d] have achieved impressive results. However, generating 3D objects under occlusion conditions is more aligned with the needs of scene generation and still requires further exploration. Most existing methods [chu2023diffcomplete, zhou20213dcompletion, stutz2018learning3dshapecompletion, cui2024neusdfusion] model the task as 3D completion, where partial geometry is derived from images and subsequently completed using 3D generation models. Recently, some methods [wu2025amodal3r, cho2025robust] additionally use occluded images and masks as supplementary information to achieve better performance. Since 3D generation models already possess sufficient geometric priors, the bottleneck is the lack of de-occlusion priors. Image datasets, which contain more diverse occlusion patterns than 3D datasets, have not been fully utilized. We address this by decoupling the de-occlusion model and leveraging image datasets for training to enhance quality and controllability.\n2.3 Pose Estimation\nModel-based pose estimation aims to predict poses based on the given CAD model. Existing methods [zheng2023hspose, tian2020shape6dpose, wang2019nocs, zhang2022ssp] have achieved impressive performance on predefined classes. Recent works [shugurov2022osop, labbe2022megapose, wen2024foundationpose, zhang2023genpose, zhang2024omni6dpose] further extend the task to arbitrary objects with regression or diffusion models. However, they lack the size prediction when they are employed on scene generation task. CAST3D [yao2025cast] address the issue with a point diffusion model, but it lacks both interaction between objects and decoupled mechanism with conditions from different spaces. We propose a unified pose estimation diffusion model with both local and global attention mechanisms to improve accuracy.\n3 Method\nIn this work, we construct a decoupled 3D scene generation framework called SceneMaker that fully exploits existing datasets to learn sufficient open-set priors. In Section 3.1, we formulate and overview the whole scene generation framework. In Section 3.2 we introduce how to leverage image datasets for decoupled de-occlusion model in 3D object generation. In Section 3.3, we propose the unified pose estimation model and extend open-set generalization with synthetic datasets.\n3.1 Framework\nAs shown in Figure 3, given a single scene image containing multiple objects , our scene generation framework aims to generate a consistent 3D scene containing corresponding 3D objects . Our framework consists of three modules: scene perception, 3D object generation under occlusion, and pose estimation, which are formally following the subsequent automated steps.\n-\n•\nUtilize Grounded-SAM [ren2024groundedsam] to segment object masks . Apply the mask on the scene image to obtain occluded object images .\n-\n•\nUtilize MoGe [wang2025moge] to estimate scene depth map . Apply mask on the depth and project pixels into 3D space to obtain point clouds .\n-\n•\nAcquire de-occluded object images with , where denotes our decoupled de-occlusion model and denotes timesteps in diffusion models.\n-\n•\nGenerate 3D object geometry based on de-occluded images with , where denotes the 3D generation model.\n-\n•\nEstimate object poses based on point clouds, images and object geometry with , where denotes the pose estimation model. Here the object poses contain rotation, translation, and size: .\n-\n•\nComposite generated object geometry and estimated poses into the final scene: .\nIn this formulation, we construct the decoupled 3D scene generation framework that fully exploits existing datasets to learn sufficient open-set priors.\n3.[ADDRESS_REMOVED] Generation with De-occlusion Model\nAfter obtaining the depth map and segmentation masks from the scene perception module, we aim to generate 3D objects with the high-quality geometry based on occluded object images. However, existing methods often struggle to generate high-quality geometry under severe occlusion. The main challenge is that models lack sufficient open-set occlusion priors due to limited 3D datasets.\nImage datasets are significantly larger than 3D datasets, encompassing a broader range of open-set objects and more diverse occlusion patterns. Therefore, compared with existing methods, we further decoupled the de-occlusion model and train it on image datasets for richer occlusion priors. The de-occlusion model is formulated as follow:\nwhere , , , denote our decoupled de-occlusion model, occluded images, de-occluded images, and timesteps in diffusion models, respectively.\nSince existing 3D native object generation models [zhao2025hunyuan3d, zhang2024clay, li2024craftsman, xiang2025trellis] have achieved impressive performance, we simply adopt existing methods [li2025step1x] for image-3d generation after de-occlusion, as shown in Equation 2:\nwhere and denote the 3D generation model and generated 3D objects, respectively.\n3.2.1 De-occlusion Model\nWe finetune Flux Kontext [labs2025fluxkontext] on our de-occlusion datasets to obtain the de-occlusion model. To acquire sufficient open-set priors and a strong understanding of natural language prompts, we directly employ Flux Kontext [labs2025fluxkontext] as the initialization for our de-occlusion model. Although both editing [labs2025fluxkontext] and inpainting [ju2024brushnet] models can achieve de-occlusion, their performance is often suboptimal in cases of severe occlusion. The fundamental cause is the lack of diverse and severe occlusion patterns in the training data. To address this issue, we construct an additional 10K object image de-occlusion dataset for finetuning to further enhance its de-occlusion capability.\nDe-occlusion Datasets. The curation pipeline is shown in Figure 4. We first use GPT [achiam2023gpt] to generate detailed captions of objects, and then employ an image generation model [flux] to produce high-quality target images. Considering that occluded images are derived from the segmentation model [ren2024groundedsam] based on predefined class labels [liu2024groundingdino], we generate 20 captions per class, and further expand them as detailed as possible to ensure high-quality images. Meanwhile, we create a universal template as the de-occlusion text prompt for all classes. Next, we carefully design three masking strategies to simulate real-world occlusions: object cutouts without background for object occlusion, right-angle cropping for image borders, and random brush strokes for user prompts, as shown in Figure 5. We also random resize the object and the whole image to simulate patterns of small objects and low-resolution images. Finally, the de-occlusion dataset is constructed by 10K triplets formed by masked images, text prompts, and target images.\n3.2.2 Comparison\nDe-occlusion. We conduct both quantitative and qualitative experiments to demonstrate the superiority of our de-occlusion model. We mainly compare our model with the state-of-the-art methods in image painting [ju2024brushnet] and image editing [labs2025fluxkontext]. We evaluate these methods on our collected validation set of 1K images spanning over 500 classes. We use PSNR and SSIM between the prediction and ground truth images, as well as the CLIP score [clip] between the prediction image and class labels, as evaluation metrics. As shown in Figure 6 and Table 1, our de-occlusion model achieve better performance on both indoor and open-set scenes, especially under severe occlusions.\nObject Generation under Occlusion. To demonstrate the superiority of our decoupled pipeline, we compare it with both existing 3D native object generation methods [wu2025amodal3r] and scene generation methods [huang2024midi] on the occluded 3D object generation task. As shown in Table 2, to align with real-world occlusion patterns, we use images rendered from the 3D-Front dataset [fu20213dfront] by InstPifu [liu2022instpifu] as the test set in our quantitative experiments, which contains numerous objects of severe occlusion. We further conduct qualitative experiments on indoor and open-set scenes in Figure 7. Both qualitative and quantitative results show that our decoupled framework achieves superior performance in occluded object generation across both indoor and open-set scenes.\n3.[ADDRESS_REMOVED]’s rotation , translation , and size in the scene based on its canonical geometry . Existing methods [wen2024foundationpose, zhang2024omni6dpose, zhang2023genpose, huang2024midi, yao2025cast] mainly face three challenges. First, they often miss size prediction when they are employed in scene generation task, since object geometries are usually generated in canonical space. Second, they do not properly decouple different pose variables when interacting with scene-level and object-level features, resulting in performance degradation. To address these two issues, we propose a unified pose estimation model that incorporates both global and local attention mechanisms in Section 3.3.1. Third, existing methods often struggle on open-set scenarios due to limited datasets. We build a large-scale open-set dataset containing over 200K synthesized scenes to tackle the generalization challenge in Section 3.3.2.\n3.3.1 Pipeline\nAs shown in Figure 3, we propose a unified pose estimation model that introduces both global and local attention mechanisms specific for the scene generation task. We directly incorporate object size into the prediction and jointly estimate it with rotation and translation, to address the adaptation challenge in scene generation task. Specifically, we take scene images , scene masks , cropped object images , point clouds , and object geometries as inputs, and predict object rotation , translation , and size as outputs, where rotation is represented in 6D.\nTo improve learning efficiency, all scenes are normalized to a unified space for pose estimation. Since all pose variables can be well represented within a Gaussian distribution, we employ the diffusion model [ddpm, lipman2022flow, dit] for pose estimation from a generative perspective, where poses are denoised from Gaussian noise with the input modalities serving as conditioning signals. The final formulation can be represented in Equation 3.\nwhere , denote the pose estimation model and timestep in diffusion models, respectively.\nAs shown in Figure 3, the trainable object pose encoder and decoder are composed of MLPs. Object geometries, images, and point clouds are encoded into features using a pretrained 3D object VAE, Dinov2 [oquab2023dinov2], and a point encoder pretrained on 3D reconstruction tasks, all of which are kept frozen during training. Object geometry is injected through concatenation with pose tokens, while image and point cloud features are injected via cross-attention. We implement our model using a flow matching framework [lipman2022flow] with a DiT architecture [dit], where each transformer block consists of global and local self-attention, global and local cross-attention, and a feed-forward network.\nAttention Mechanisms. As shown in Figure 8, we adopt both global and local mechanisms for self-attention and cross-attention. Each pose variable is separately encoded as a token, so each object in the diffusion model is uniquely represented by a quadruple of tokens: rotation, translation, size, and geometry. The local self-attention module enables the interaction inside the quadruple of each object. The global self-attention module enables tokens of all objects in the scene to interact with each other, leading to more coherent relative object poses. Considering that rotation can be independently estimated in the object canonical space and scene-level conditions provide little benefit, we introduce a local cross-attention module, allowing the rotation token to attend only to the cropped object image and normalized object point cloud. Meanwhile, we retain a global cross-attention module for the translation and size tokens, allowing them to attend to the scene-level point cloud and image. This fine-grained attention mechanism is demonstrated effective in our comprehensive experiments.\n3.3.2 Open-set Scene Datasets\nSince existing datasets currently lack the necessary prior for training a 3D scene generation model in an open-set domain, we addressed this by constructing our own training data. This involved using a carefully curated subset of the existing Objaverse [deitke2023objaverse] dataset along with Blender [blender]. A significant number of models in Objaverse are either scanned data or have low-quality textures and materials, which necessitated a rigorous curation process. To filter the models, we assessed their material information, excluding any that were transparent, lacked a BSDF node, or did not have an albedo map. To further refine the selection, we also excluded models with pure or excessively dark albedo colors. Ultimately, this process resulted in a high-quality subset of 90k models with a superior appearance to construct a dataset of 200k scenes for our work.\nWe composed each scene by combining 2 to 5 randomly selected objects. To enhance realism, we used random environment maps sampled from Polyhaven [Polyhaven] to serve as the background of the scenes. Additionally, we added a ground plane with a high-quality texture beneath the objects, using Perlin noise to enhance the surface and add realistic variations. Finally, each object was given a random rotation to serve as an augmentation of the level of the object to train the pose estimation module.\nEach scene is rendered using Blender’s CYCLES engine from 20 viewpoints in 512 resolution, with camera elevations randomly sampled between degrees. Simultaneously, we also uniformly sampled 20k random points on the object’s surface to serve as the input geometric information for our object generation module. We also augment the background of images with random selection. To ensure physical plausibility, we place the lowest point of each object on the same plane and enforce that their bounding boxes do not intersect. We present samples from our dataset in Figure 9. We randomize the pitch angle of input meshes during training to better align 3D object generation outputs. This entire process resulted in a dataset of 200k scenes, comprising a total of 8 million images.\n3.3.3 Training\nWe directly apply L2 loss to rotation, translation, and size, with equal weighting for each term. To demonstrate the superiority of our framework, we first train our model only on the 3D Front datasets [fu20213dfront] for fair comparison. We mix the datasets curated by MIDI3D [huang2024midi] and Instpifu [liu2022instpifu]. We align their render results according to room IDs, resulting in 20K scenes. We take 1K scenes as test sets and the rest as training sets. We train the model from scratch for 25K steps. To extend the generalization on open-set, we further mix our 200K open-set datasets into the indoor datasets, and take 1K scenes as open-set test sets. We train the model from scratch for 40K steps until the model converged.\n4 Experiments\n4.1 Settings\nDatasets and Baselines. We conduct experiments on both indoor and open-set datasets. Specifically, we conduct quantitative comparisons with existing methods [liu2022instpifu, huang2024midi, ardelean2024gen3dsr, nie2020total3dunderstanding, gao2024diffcad, han2025reparo, chen2024ssr, dahnert2021panoptic] on the MIDI [huang2024midi] test set containing 1K scenes to demonstrate the superiority of our framework. To further validate the generalization of our method under severe occlusions and open-set scenarios, we randomly select 1K scenes with no overlap with the training set from 3D-front [fu20213dfront] as indoor test sets, and 1K scenes from our collected open-set data as open-set test sets. It is worth noting that our 3D-Front scenes contain significantly more occlusions compared to MIDI test set. We conduct both quantitative and qualitative comparison with SOTA methods MIDI [huang2024midi] and PartCrafter [lin2025partcrafter]. We also conduct qualitative comparisons on synthetic, in-the-wild, and real-world captured images from a wider range of application domains.\nMetrics. Following existing scene generation methods [yao2025cast, huang2024midi, lin2025partcrafter], we use scene-level Chamfer Distance (CD-S), F-Score (F-Score-S), and IoU Bounding Box (IoU-B) to evaluate the quality of the whole scene. And we use object-level Chamfer Distance (CD-O) and F-Score (F-Score-O) to evaluate the quality of generated object geometry.\n4.2 Quantitative Results\nAs shown in Table 4, we conduct a quantitative evaluation on the MIDI test set with a wide range of existing methods [liu2022instpifu, huang2024midi, ardelean2024gen3dsr, nie2020total3dunderstanding, gao2024diffcad, han2025reparo, chen2024ssr, dahnert2021panoptic], and our approach achieves the best overall performance. As shown in Table 3, our method consistently outperforms existing SOTA methods [lin2025partcrafter, huang2024midi], achieving the highest metrics on the more challenging indoor and open-set scene generation tasks. Remarkably, even without being trained on the open-set dataset, our method still obtains the best quantitative results on indoor scenes, which underscores the superiority of our proposed framework and designed modules.\n4.3 Qualitative Results\nAs shown in Figure 10, our method generates visually compelling scenes that are not only realistic but also rich in detail. Crucially, our model demonstrates a robust ability to handle severe occlusions in Figure(a)(b), accurately reasoning about the relative spatial relationships between objects and places objects in plausible poses in Figure(c)(d)(f). Besides, our model can also handle small objects without geometry degradation in Figure(e).\nWe conduct more qualitative comparisons on both indoor and open-set scenes in Figure 13 and Figure 14, including both synthetic and real-world captured images. Our method offers better generalization to open-set scenes. Moreover, it delivers more accurate poses and finer geometry under severe occlusion or for small objects.\n4.4 Ablation Study\nAttention Mechanism. We ablate the contribution of the global and local self-attention mechanism, and the decoupled cross-attention mechanism in the pose estimation model respectively. For the self-attention mechanism, we simply remove the global and local attention modules respectively for comparison. For the decoupled cross-attention mechanism, we remove the local attention and merge the rotation update into the global attention for comparison. We train the above models from scratch and use ground-truth meshes to eliminate the influence of geometry on pose estimation. As shown in Table 5, all modules in our proposed attention mechanisms contribute positively to the performance, which underscores their superiority.\nGeneralization on number of objects. As shown in Figure 11, we conduct experiments scene images containing varying numbers of objects to demonstrate the model’s generalization capability. Although each scene in the training set contains no more than five objects, thanks to the design of RoPE [su2024rope], our pose estimation model generalizes well to scenes with more than five objects.\nOpen-set Datasets. We demonstrate the necessity of our proposed scene datasets on the open-set images as shown in Table 3. Our model faces severe degradation in open-set scenario without the datasets. The datasets mainly provide open-set patterns of diverse objects, which help build pose mappings across different geometries and are essential for open-set scene generation.\nControllable object generation. Benefiting from our decoupled de-occlusion model, compared with 3D native methods [wu2025amodal3r, huang2024midi], our model further enables controllable generation of the occluded areas of objects through prompts. As shown in Figure 12, our model is able to control the color of the pot and things in the penguin’s hand through prompts during de-occlusion.\nUpper Bound of Pose Estimation. Compared to a single image, videos or multi-image can provide richer scene structure information through point cloud reconstruction. When the reconstruction algorithm [wang2025vggt, wang2024dust3r] reaches its upper limit, it is equivalent to providing our model with a complete point cloud. We discuss the upper bound of our pose estimation model by giving the complete point clouds. As shown in Table 5, with a complete point cloud, our model achieves a significant performance boost, demonstrating its strong potential under video or multi-image conditions.\n5 Conclusion\nIn this paper, we propose a decoupled 3D scene generation framework called SceneMaker. To obtain sufficient occlusion priors, we decouple and develop the robust de-occlusion model from 3D object generation by leveraging image generation models and a 10K curated de-occlusion dataset for training. To improve the accuracy of the pose estimation model, we propose a unified pose estimation diffusion model with both local and global attention mechanisms. We further construct a 200K synthesized scene dataset for open-set generalization. Comprehensive experiments demonstrate the superiority of our framework on both indoor and open-set scenes.\nLimitations and future work. Although our framework effectively generalizes to arbitrary objects, the real-world arrangement of objects is often much more complex than what our datasets capture, particularly when force interactions are involved. Therefore, a key future research topic is how to construct or refine 3D scenes more accurately in a physically plausible manner, including interpenetration and force interactions. Meanwhile, existing methods can only control scene generation through images or simple captions, and further development is needed for more control signals and natural language interactions. Moreover, how to perform more in-depth understanding tasks and adapt embodied decision-making based on generated high-quality 3D scenes is also an unsolved challenge."
},
{
  "article": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision\nAbstract\nThe success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.\nWe present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.\n1 Introduction\nEmbodied visual navigation (mapping pixels to velocity/acceleration) in urban environments is a popular area of research due to its capacity to enable direct, end-to-end deployment, which is useful for real-world applications such as last mile delivery. Despite rapid progress, visual navigation performs poorly in dynamic and unstructured environments, which involve dense and diverse pedestrian movements, irregular roadside configurations, and an open-world variety of object categories. Navigating dynamic unstructured environments effectively requires a comprehensive and accurate understanding of 3D scene semantics, geometry, and dynamics, adherence to common-sense rules and social conventions, such as using sidewalks, obeying traffic signals, and keeping appropriate interpersonal distance.\nEarly visual navigation approaches [muhlbauer2009navigation, kummerle2013navigation, morales2009autonomous, guan2022ga] build on modular vision systems for detection, tracking, and planning, yet their performance was constrained by hard-coding rule-based decision making and evaluations confined to simple or near-static environments. Reinforcement learning in photorealistic simulators [savva2019habitat, xia2018gibson, kolve2017ai2, chang2017Matterport, wu2025urbansim, xie2024vid2sim, he2025from, wu2025metaurban] achieved progress but still suffers from the sim-to-real gap. Recent efforts [shah2023gnm, shah2023vint, sridhar2024nomad] introduced visual navigation foundation models (NFMs) that learn from expert demonstrations to map visual inputs directly to actions, however, their scalability remains constrained by the scarcity of pixel-to-action supervision. More recently, CityWalker [liu2025citywalker] addressed this limitation by automatically mining and annotating human navigation videos from the Internet. Although NFMs trained on such in-the-wild data achieved impressive improvements, their robustness in dynamic and unstructured environments remains poor for two reasons: NFMs use monocular input which introduces depth-scale ambiguity resulting in annotation noise and performance bottlenecks. For real-world robot navigation, accurate depth is critical for safety [de2024point, chen2025livepoint], which is why most robotic platforms typically include stereo cameras and NFMs assume that mid-level vision priors, which are necessary for providing structure to the predicted outputs, would emerge implicitly. However, this is a strong assumption and prior work in other robotic domains [zhou2019does, sax2019learning, chen2020robust, yen2020learning, muller2018driving, mousavian2019visual, yang2018visual] has concluded that explicit mid-level supervision improves performance.\nTherefore in this work, we propose to empower NFMs with stereo and mid-level vision. Our work is guided by two key insights: First, stereo inputs resolve the depth–scale ambiguity inherent in monocular perception. Adding stereo input improves navigation success and reduces error in a number of critical navigation scenarios. Second, mid-level vision improves generalization, stability, and data efficiency. StereoWalker surpasses the stat-of-the-art, CityWalker [liu2025citywalker], using only of its training data (Fig. 1), highlighting the impact of stereo and mid-level vision on scalable and robust urban navigation.\nMain Contributions: In summary,\n-\n•\nWe present StereoWalker, a visual NFM built on stereo inputs and structured with explicit mid-level vision modules, achieving state-of-the-art performance in overall navigation.\n-\n•\nWe release a new stereo dataset of pedestrians walking in global metropolitan cities. We additionally develop a filter using vision-language models removing contents that do not contain goal-directed walking.\n-\n•\nWe demonstrate our approach on established benchmark (CityWalker), our new benchmark (StereoWalker), and in real world environments.\n2 Related Work\nEmbodied Navigation Tasks.\nEmbodied navigation tasks are commonly categorized by how the goal is specified. Existing paradigms include point-goal or position-goal navigation [chaplot2020learning, chattopadhyay2021robustnav, gordon2019splitnet, jaderberg2017reinforcement, gupta2017cognitive, savva2019habitat, Wijmans2019DDPPO], image-goal navigation [mezghan2022memory, ramakrishnan2022poni, zhu2017target, savinov2018semiparametric, hahn2021no, kwon2021visual, krantz2023navigating], object-goal navigation [al2022zero, chang2020semantic, chaplot2020object, gervet2023navigating, majumdar2022zson], and vision–language navigation [das2018embodied, tan2019learning, krantz2020beyond, qi2025vln, zhang2024navid, zhang2024uni]. Our work follows the position-goal formulation, where navigation is defined through ordered waypoints in Euclidean space. While in public urban spaces, recent foundation models for visual navigation [shah2023gnm, shah2023vint, sridhar2024nomad, liu2025citywalker] have shown encouraging performance across these paradigms, they often abstract away the fine-grained visual structure of the scene, relying on compressed global features or language-conditioned embeddings. In contrast, the embodied navigation task inherently depends on rich visual understanding (e.g., geometry, motion, and spatial layout), all of which are observable in the image stream but only partially exploited in most current approaches. The focus of our formulation is therefore on examining how a model can more completely utilize visual information to infer spatial transitions between successive waypoints.\nMid-level Vision for Robotics.\nPrior studies [chen2020robust, yen2020learning, muller2018driving, mousavian2019visual, yang2018visual, zhou2019does, sax2019learning, yao2024openvocabularymonocular3d, yao2025labelany3d] have shown that mid-level visual representations can improve generalization and data efficiency across a wide range of robotic tasks. Early works [chen2020robust, yen2020learning, muller2018driving, mousavian2019visual, yang2018visual] demonstrated that supplying sensorimotor policies with geometric and semantic cues such as optical flow, depth, and segmentation leads to better action prediction. Subsequent efforts [zhou2019does] expanded this idea to diverse tasks in simulation, showing that mid-level features often outperform raw pixels and reduce sample complexity. Other approaches [chen2020robust] explored their use in manipulation, grasping, and navigation, highlighting their value relative to strategies like domain randomization. However, most of these studies were conducted in simplified or static environments and preceded the emergence of navigation foundation models. Our work revisits this line of inquiry in the context of real-world dynamic urban navigation and provides evidence that explicit mid-level vision remains essential for scalable and reliable end-to-end navigation models.\nStereo Vision for Robotics.\nModern navigation systems incorporate diverse sensing modalities such as LiDAR for accurate three dimensional geometry [xu2025lv, rashed2019fusemodnet, zhou2023lidar], event cameras for high frequency motion cues [gallego2018unifying, he2024microsaccade], and multi camera setups for broad spatial coverage [li2024bevformer, jiang2023vad]. Stereo cameras offer a practical alternative, providing accurate depth with passive sensing, low cost, and simple deployment, which has led to their broad use in robotics [shi2024asgrasp, kolter2009stereo]. Stereo perception supports a range of robot learning tasks, including manipulation and grasping [shankar2022learned, khazatsky2024droid, bai2024cleardepth, cheng2024open], industrial assembly and insertion [spector2022insertionnet, bartyzel2023reinforcement, ma2024cross, chen2023intelligent], and dynamic locomotion and whole body control [yin2025visualmimic, clarke2025x, li2025amo]. Our work leverages stereo video as large scale training data for navigation, using geometric cues as implicit visual context rather than direct sensory measurements.\n3 Stereo Urban Navigation Dataset\nWhile stereo sensing is widely used in robotic perception, it remains underexplored in embodied urban navigation, where most visual models rely on monocular or depth-simulated imagery. To bridge this gap, we curate a large-scale training dataset from stereoscopic walking videos mined from publicly available YouTube content under the Standard License. We focus on high-resolution VR180 first-person videos, which naturally provide egocentric stereo geometry suitable for learning visuomotor representations.\nOur dataset includes roughly 500 independent and non-overlapping clips, totaling hours of stereo footage spanning multiple global cities such as San Francisco, Madrid, and Tokyo. Compared to the monocular data in CityWalker [liu2025citywalker], which was captured within a single metropolitan region, our collection offers broader diversity in architectural layouts, lighting, weather, and pedestrian density. Each video is filtered and rectified into left-right image pairs indexed by frame , to ensure reliable geometric correspondence. Following practices in web-mined video datasets such as Stereo4D [jin2025stereo4d], we will release annotations, metadata, and video links under a CC license.\nFiltering and Quality Control.\nWe apply an automatic filtering stage to ensure that the collected videos capture goal-directed walking rather than passive observation or unrelated activities. Online walking footage often includes segments where the camera wearer pauses, interacts with bystanders, or engages in activities such as shopping or sightseeing.\nCityWalker videos contain similar non-navigational content, which can introduce undesirable biases into learned navigation behaviors (see supplementary material). To remove these segments, we use a vision-language filtering model, Qwen2-VL [wang2024qwen2], which analyzes visual content together with temporal context. For each candidate clip, the model reviews frames sampled at one frame per second, along with associated captions, and assesses whether the motion reflects forward locomotion toward an implicit goal. Only clips that consistently display egocentric, target-oriented walking are retained. The full filtering prompt is provided in the supplementary material.\nThe resulting dataset consists mainly of continuous walking sequences with clear ego-motion and minimal idle behavior. These filtered clips serve as the input to the subsequent trajectory estimation stage based on stereo visual odometry. We include precision and recall metrics to demonstrate the efficiency of our filtering approach in the supplementary material.\nAction Labels from Videos.\nTo derive action supervision for training our navigation foundation model, we compute trajectory labels directly from the collected stereo videos using state-of-the-art stereo visual odometry (VO) methods. In particular, we adopt MAC-VO [qiu2025mac], which utilizes stereo input and surpasses other VO methods and SLAM in challenging scenarios. The resulting trajectories give higher quality camera translation compare to DPVO [teed2024deep] which is used by CityWalker, as illustrated in Figure 2. This VO-based labeling pipeline can process large quantities of raw stereo footage without manual annotation or language-based prompting, enabling efficient expansion to large scale training data.\n4 StereoWalker\nOur model fuses mid-level vision foundation models and utilizes stereo information. In this section, we first discuss the task setting (Sec. 4.1); we then provide the details of StereoWalker (Sec. 4.2).\n4.1 Preliminaries: Dynamic Urban Navigation\nWe study visual navigation in dynamic urban settings, where the objective is to generate a series of waypoints from the current location to a specified target. Urban environments introduce substantial challenges due to their complex visual structure and human and vehicle motion, which demands an understanding of geometry and dynamics from raw visual input. Following the task setup of previous urban navigation foundation models [liu2025citywalker, sridhar2024nomad, shah2023vint, shah2023gnm], each training instance consists of a temporally ordered sequence of observations , corresponding positions and a sub-goal waypoint . The learning objective is to approximate the following neural function:\nwhere denotes the length of a short temporal window of recent stereo observations and positions. Our model takes the current subgoal as the immediate target and predicts the short-horizon trajectory that advances toward it. Once the model reaches , the next waypoint in the predefined sequence becomes the new sub-goal, forming a continuous waypoint-to-waypoint navigation process. This formulation can naturally scale to long-range navigation by chaining multiple waypoints obtained from a global path planner such as and other graph-search based algorithms.\n4.2 Model Design\nOverview.\nWhile prior visual navigation models compress each frame to a single DINOv2 [CLS] token [oquab2023dinov2], StereoWalker retains all patch tokens to preserve fine-grained spatial structure critical for control. Our intuition is straightforward: accurate navigation demands richer visual perception than a global summary can provide.\nAs shown in Fig. 3, given a short temporal window of rectified stereo (or monocular) frames and their corresponding positions , the model forms dense mid-level tokens that jointly encode appearance, geometry, and short-term motion cues. Tokens from all frames are then processed by three stages: tracking-guided attention to maintain temporal correspondence and reduce drift, global attention to integrate scene context across views, and target-token attention to focus prediction on goal-relevant regions. StereoWalker supports both stereo and monocular inputs with the same architecture, differing only in tokenization.\nImage tokenization.\nWe employ three off-the-shelf foundation vision models to obtain complementary information from each frame in . Specifically, DINOv2 [oquab2023dinov2] provides high-level patch representations, DepthAnythingV2 [yang2024depth] estimates per-pixel depth and CoTracker-v3 [karaev2025cotracker3] generates point trajectories across time, where is the number of tracks. All the pretrained models are frozen in our architecture.\nDepth aggregation.\nWe design our depth module to be compatible for both monocular and stereo input. While DINOv2 computes hierarchical visual features, DepthAnythingV2 leverages these features to predict a per-pixel depth map . For stereo image pairs , DepthAnythingV2 is also applied to the left image for the depth map, which is used to get a refined disparity map between the left and right views with a pretrained stereo matching network of MonSter++ [cheng2025monster]. The geometric relationship between disparity and depth is given by , where denotes the focal length and is the stereo baseline. The resulting disparity map is converted to a depth map, which is then patchified by a depth encoder into depth embeddings , with and indexing the spatial location of each patch in the image. We treat depth as an independent feature dimension rather than as a derivative of RGB texture. Accordingly, each image token is f by concatenating the DINO-derived appearance embedding and the depth embedding, producing that jointly encodes photometric and geometric information.\nTracking-guided attention.\nTo capture temporal correspondence across frames, we introduce a tracking-guided attention module inspired by TrackTention [lai2025tracktention]. Given image tokens for each frame within a temporal window and a set of point tracks obtained from CoTracker-v3, where denotes the number of tracks. This module integrates spatial appearance and temporal motion through three successive operations.\n1) Track-aware sampling. Each 2D point is embedded into a track token through positional encoding and projection. Using cross-attention between the track tokens and image tokens, the model aggregates local visual evidence from into a set of sampled track features . This operation allows each track to selectively pool information from nearby spatial regions based on learned attention weights rather than fixed interpolation.\n2) Temporal propagation. The track features from consecutive frames are organized into sequences of length for each track and processed by a transformer that performs self-attention along the temporal axis. The resulting representations capture smooth temporal evolution of appearance and motion, reinforcing correspondence across occlusion or viewpoint changes.\n3) Feature update. To update image features, we perform a second cross-attention where spatial coordinate embeddings act as queries and the updated track tokens serve as keys and values. This produces motion-aware corrections that are added back to the image tokens through a residual connection, . The resulting features encode both static structure and dynamic correspondence, providing motion-consistent representations for downstream reasoning.\nGlobal attention and target-token attention.\nFor navigation conditioning, the sub-goal waypoint and recent trajectory are projected through a lightweight MLP to obtain corresponding trajectory tokens and a target token . After the tracking-guided attention, all image tokens, and trajectory tokens are passed into a unified sequence and processed jointly through multi-head self-attention layers, referred to as the global attention stage:\nSubsequently, the target token is introduced and processed through another set of self-attention layers (target-token attention). Here we only take the updated target token absorbing all information in past seconds. Finally, is passed through an MLP and two heads: an arrival head predicting the probability of reaching , and an action head predicting the next waypoints .\nTraining objectives.\nDuring training, we minimize a composite loss which is formulated as\nwhere denotes the waypoint prediction loss measuring spatial deviation between predicted and ground-truth trajectories, supervises the arrival probability at the target waypoint, compares the mean angle difference of each step. and are scalar weights balancing the auxiliary objectives. More details on these loss functions will be provided in the supplementary material.\n5 Experiments\nIn this section, we evaluate the performance of our proposed framework, StereoWalker, for goal-directed navigation using monocular and stereo visual inputs. We aim to address the following key questions: does explicitly incorporating mid-level vision features improve robot navigation over strong NFM baselines? does training with high quality stereo data enhance navigation robustness and accuracy compared to monocular setups? and does StereoWalker transfer reliably to real world robot deployments in a variety of critical navigation scenarios? To answer these questions, we outline the experimental setup, including baseline methods, evaluation metrics, and dataset collection, in Sec. 5.1. We then present stereo and monocular benchmarking results in Sec. 5.2. Finally, we provide detailed analysis of our results in Sec. 5.3.\n5.1 Setup\nBaselines.\nWe compare our model with outdoor navigation models closely related to our setup, including GNM [shah2023gnm], ViNT [shah2023vint], NoMaD [sridhar2024nomad] and CityWalker [liu2025citywalker]. Although originally developed for image-goal navigation, we tested GNM [shah2023gnm], ViNT [shah2023vint], and NoMaD [sridhar2024nomad] using goal-images from our collected data. We acknowledge CoNVOI [sathyamoorthy2024convoi] as a recent work with a similar setup, but could not test it due to the lack of open source code.\nEvaluation Metrics.\nWe evaluate predicted trajectories with two complementary metrics: Maximum Average orientation error (MAOE), Arrival Accuracy and Euclidean distance. Following CityWalker [liu2025citywalker], let denote the orientation (heading) error between the predicted motion direction and the ground-truth direction for sample at future step . With samples and a horizon of steps, we define\nThat is, for each sample we take the worst per-step orientation error over the horizon and then average across samples. MAOE captures the model’s ability to maintain a correct heading toward the sub-goal, independent of trajectory scale. Let be the predicted position for sample at step and the target waypoint. Given a radius and a step budget , the Arrival Accuracy is the fraction of samples that come within of the target within the next steps:\nThis metric directly reflects waypoint-level success and provides an interpretable notion of spatial goal attainment. Although Euclidean distance is a straightforward measure of positional discrepancy, prior work [liu2025citywalker] shows it can miss navigation quality, i.e., trajectories may have small error yet head in the wrong direction. Nevertheless, remains a useful indicator of absolute localization accuracy and scale, so we report it alongside MAOE and Arrival Accuracy as a complementary metric.\nData.\nWe collected expert demonstrations through teleoperation for fine-tuning and offline evaluation. The dataset was acquired using a Clearpath Jackal equipped with an Ouster LiDAR and a ZED 2i stereo camera. A LiDAR-based SLAM system was used to estimate the robot’s pose, which served as the ground-truth action labels. Additionally, wheel odometry provided continuous relative motion estimates, enabling the robot to infer its current pose and predict its future trajectory. We also evaluated our model in a monocular setting using the CityWalker benchmark [liu2025citywalker] in monocular settings.\n5.2 Performance Benchmarking\nWe benchmark our model and strong baselines on the CityWalker benchmark and on our own stereo benchmark. In addition, we validate the findings with real-world navigation deployments.\nMonocular Benchmark.\nTable 1 reports results across critical scenarios. Overall, our fine-tuned model improves performance in most categories, improving MAOE by an average of 4–13%, and arrival rates by 1–19%. Fine-tuned StereoWalker achieves the best scores on many metrics, in every scenario except the “turn” case. We hypothesize that the relative weakness on turns arises from data imbalance toward straight segments and the amplification of small orientation errors during sharp heading changes. Notably, explicitly adding mid-level vision features, including depth and tracking, improves performance on most categories, underscoring the value of structured geometric and temporal cues for monocular navigation. We provide more in depth analysis of each modality in Sec. 5.3.\nStereo Benchmark.\nTable 2 summarizes results across critical scenarios on our offline StereoWalker benchmark. Because our baselines do not natively ingest stereo, we feed their left image and also train a monocular variant of our model for a fair comparison. Even without stereo, our monocular model already yields substantial gains, reducing average L2 error by 17–73%, MAOE by 11–48%, and arrival rates by 3–24%. Training with stereo further achieves SOTA performance with consistent gains in Crossing, Detour, Crowd, and Other. The Turn scenario remains challenging, with arrival lagging behind the monocular baselines despite comparable orientation error, suggesting the need for more turning-rich data or turn-aware objectives. Overall, stereo supervision provides a clear benefit beyond strong monocular baselines, reducing overall L2 error by 18–73%, MAOE by 22–54%, and arrival rates by 3–25%.\nReal-world Deployment.\nWe evaluate our model in real-world setting using a Clearpath Jackal J100 robot. The two strongest models on offline benchmarks, our model and CityWalker [liu2025citywalker] are tested with identical start and end points to ensure comparability. Each model runs remotely on a GPU server and communicates with the robot through FastAPI, which streams the predicted waypoints. The low-level controller on ROS2 converts the predicted short-horizon trajectory into continuous velocity commands for execution. We evaluate three motion patterns including forward, left turn, and right turn, with 14 trials conducted for each case. A trial is considered successful when the robot arrives within 1m of the designated target and stays in that area; interruptions due to collisions are marked as failures. The consistent improvements across all motion types suggests that our model architecture provides more stable waypoint estimation under dynamic conditions, and the stereo training data boosts the performance further.\n5.3 Analysis\nAnalysis of Mid-level Vision.\nTable 3 presents an ablation analysis that evaluates different architectural configurations on the CityWalker teleoperation benchmark. All variants are trained on the same monocular dataset, with specific components selectively enabled or disabled for a fair comparison. Earlier baselines such as ViNT [shah2023vint], GNM [shah2023gnm], NoMaD [sridhar2024nomad], and CityWalker [liu2025citywalker] represent each image using only a single [CLS] token. In contrast, we observe that using all patch tokens to capture finer spatial information leads to an immediate 3.7% improvement in the Mean Angular Orientation Error (MAOE). Building upon this representation, we observe that incorporating depth and dense pixel tracking further enhances navigation accuracy, as these two mid-level cues provide complementary inductive signals. Depth captures the three-dimensional structure, and adding depth yields a further 4.0% reduction in MAOE relative to the patch token model.\nTracking encodes scene motion and temporal consistency, and incorporating tracking on top of patch tokens and depth provides an additional 2.8% reduction in MAOE. Prior studies [chen2020robust, zhou2019does] demonstrated similar advantages of mid-level vision in controlled or static environments. Our experiments in large-scale dynamic urban navigation, showing that explicitly modeling depth and motion significantly improves robustness and effectiveness in real-world conditions, shown in Fig. 4. Fine-tuned StereoWalker improves performance by an average of 23.8% over the Forward, Left turn, and Right turn scenarios. Each design takes 10 epochs and shares the same hyperparameter settings.\nAnalysis of Training Efficiency.\nBeyond performance gains, we also observe that incorporating mid-level vision capabilities significantly accelerates training. We carefully train our model on monocular data using only a fraction of the original dataset, achieving comparable performance with merely 1.5% of CityWalker’s training data. Under different amounts of training data, we use the same training settings for both CityWalker and our model. Enabling patch tokens introduces richer visual representations but also necessitates architectural modifications to the decoder, as our model no longer relies on a single [CLS] token representation. Consequently, our model with patch tokens alone does not surpass CityWalker when trained for 30 hours. However, once depth cues are injected, the model already outperforms CityWalker. Further incorporating both depth and tracking information leads to faster convergence and superior performance, surpassing CityWalker trained with over 2,000 hours of monocular videos. This demonstrates that mid-level vision not only enhances representation quality but also provides strong inductive biases that make training more data- and time-efficient.\n6 Conclusion, Limitations, and Future Work\nSummary.\nIn this work, we present a visual NFM that integrates stereo inputs with explicit mid-level vision modules for dynamic urban environments. To support this model, we collect rectified stereo videos from Internet VR180 footage, paired with an automatic filtering process. Across curated benchmarks and real-world tests, our model achieves state-of-the-art performance with remarkably training efficiency, and we analyzed the effectiveness of stereo and mid-level vision, indicating the continued relevance of core computer vision representations in the development of end-to-end robotic navigation models.\nLimitations.\nWhile StereoWalker demonstrates the utility of stereo and mid-level vision for embodied navigation, these ideas are not yet fully explored across the broader landscape of robotic tasks. Many embodied systems, ranging from mobile manipulators to aerial robots, could benefit from structured geometric cues and motion-aware visual representations, yet the space of mid-level vision for general-purpose robot learning remains largely open. We view our work as an initial step toward this direction, and anticipate that future models trained on larger and more diverse multi-robot datasets may yield broader generalization and more flexible capabilities.\n7 Acknowledgment\nThe authors acknowledge the Adobe Research Gift, the University of Virginia Research Computing and Data Analytics Center, Advanced Micro Devices AI and HPC Cluster Program, Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, and National Artificial Intelligence Research Resource (NAIRR) Pilot for computational resources, including the Anvil supercomputer (National Science Foundation award OAC 2005632) at Purdue University and the Delta and DeltaAI advanced computing resources (National Science Foundation award OAC 2005572). PI Chandra is supported in part by a CCI award supported by the Commonwealth of Virginia.\nSupplementary Material\nSec. A reports the evaluation metrics and provides detailed statistics of the DIVERCITY dataset. Sec. B presents additional training details for StereoWalker, including the full set of hyperparameters, our implementation of the tracking-guided attention module, as the original Tracktention work [lai2025tracktention] did not release source code, and an extended ablation study. Sec. C further describes real-world deployment settings and includes supplementary qualitative visualizations.\nAppendix A DIVERCITY Data Curation\nFor the collection of stereo walking videos, we curate footage captured across multiple cities, extending beyond prior datasets such as CityWalker [liu2025citywalker], which its data is curated on videos from a single creator and limited to New York City. All videos are downloaded in VR180 format and subsequently rectified using the pipeline provided by Stereo4D [jin2025stereo4d], producing synchronized left and right perspective views at a resolution of and a frame rate of 30 fps. Moreover, CityWalker contains non-navigation content as shown in Fig. 7. For example, the creator will stop and watch events and etc. To remove such noisy from our dataset, we adopt Qwen2-VL [wang2024qwen2] to filter all the videos collected using the prompt: “Is this a first-person video of a person actively walking on foot (not standing still)? Answer strictly with ‘yes’ or ‘no’.” Based on the feedback from Qwen2-VL [wang2024qwen2], we filter out clips with the answer of ’no’. As a result, 544 of 748 video clips are selected as valid navigation clips. In addition, we verify the process of filtering by hand annotating 100 video clips and obtained a precision of 100% and a recall of 97.5%, which demonstrate the reliability and effectiveness of this video filtering strategy.\nFor additional quality control and action label acquisition, please refer to Sec. 3.\nAppendix B Experimental Details\nIn this section, we introduce details of StereoWalker. Sec. B.1 provides detailed settings used in training; Sec. B.2 demonstrates the complete process of tracking-guided attention; Sec. B.3 gives the performance in all subsets of the benchmark for ablation study.\nB.1 More Training Details\nWe train StereoWalker under the hyperparameter settings summarized in Tab. 4. The model is optimized for ten epochs using AdamW with an initial learning rate of and a cosine decay schedule, and a reduced learning rate of during fine-tuning. Training is conducted on four NVIDIA A100 GPUs with 80GB memory, resulting in a total wall-clock time of approximately five hours.\nFor the mid-level vision components, we adopt lightweight and fast-inference variants of state-of-the-art models to balance computational efficiency and representational quality. Specifically, RT-MonSter++ [cheng2025monster] is used as the binocular depth estimator to produce high-quality disparity and depth maps from stereo inputs, while CoTracker3 online [karaev2025cotracker3] is applied for point tracking to provide temporally coherent motion cues across frames. For monocular depth estimation, we employ Depth-Anything-V2-Base [yang2024depth], which directly operates on hierarchical features extracted from DINOv2 with a ViT-B/14 backbone with feature dimension of 768.\nAll inputs are resized to a resolution of . While the depth embedding dimension is 64. These are concatenated with DINO feature to form image tokens with an attention hidden dimension of 832. The network comprises two tracking-guided attention layers, twelve global attention layers, and four target-token attention layers. Each training sample uses a temporal context of five past frames and predicts five future waypoints at a frequency of 1 Hz, along with an arrival probability.\nThe loss function weights are set to and . Empirically, we find this hyperparameter provides better directional consistency in critical scenarios.\nB.2 Implementation of Tracking-guided Attention\nAlthough our tracking-guided attention is inspired by Tracktention [lai2025tracktention], their code has not yet released. We now provide the details of our design of this module and will public the implementation for reproducing our results.\nFollowing Tracktention [lai2025tracktention], let be the patch position in an image, be the height and width of the patchified image. For one patch, denotes DINO-derived appearance embedding, and denote depth embedding. We then get patch tokens that concatenates these two embeddings. Tracking-guided attention takes as input the tokens for each frame in the window , and a set of point tracks produced by the point tracker, where is the number of tracks.\nWe first convert the 2D track positions into track tokens by applying a 2D RoPE with regard to its patch position and a linear projection ,\nand arrange them into a tensor . The image tokens for frame are stacked as and also applied RoPE on. We then use cross-attention to pool information from image tokens into the track tokens,\nwhere are track features that summarize the local neighborhood of each track in the feature map. Intuitively, this stage samples feature information around each track location using a learnable, attention-based weighting rather than fixed interpolation.\nTo aggregate information over time, we apply a transformer along the temporal dimension of each track. Let denote all track features in the window, which we rearrange into a tensor of shape by treating each track as a short sequence in time. A track transformer is applied independently to each track sequence,\nyielding updated track tokens that carry temporally smoothed information along the corresponding trajectories.\nIn the final stage, we redistribute the updated track features back to the image token grid. We construct queries from spatial coordinate embeddings associated with each image token and use the updated track tokens as keys and values,\nwhere and the output encodes motion-aware corrections for each image token. As a result, the final tokens incorporate both local appearance and trajectory-aligned motion information, while preserving the spatial layout of the original feature map.\nDifferent from Tracktention paper, which alternates ViT and Tracktention blocks multiple times, we employ only two tracking-guided attention layers at the beginning of the transformer for efficiency, since we are approaching from the robotics side.\nB.3 More Ablation Details\nIn Tab. 5, we report the performance of different architectural variants across all evaluation scenarios. The full StereoWalker model achieves the best MAOE on average and overall. We observe that replacing the CLS token with dense patch tokens and introducing mid-level vision modules consistently improves performance across most settings, indicating their effectiveness in enhancing waypoint prediction quality. Specifically, the addition of depth and tracking improves performance to different extents across scenarios, further illustrating the complementary inductive information contributed by these components.\nWe also observe that in the Detour scenario, where a slight performance drop is observed. As also discussed in CityWalker, this subset of the benchmark contains a relatively small number of samples, making the results more sensitive to data imbalance. Notably, both CityWalker and our model exhibit substantial gains after fine-tuning on this scenario, with CityWalker improving from 16.3 to 13.9 MAOE and our model from 17.0 to 13.0, suggesting that increased data or targeted fine-tuning can significantly mitigate this limitation.\nAppendix C Real-world Deployment Details\nIn this section, we give more analysis on the real-world deployment of StereoWalker. Similar to other urban visual navigation models [shah2023gnm, shah2023vint, sridhar2024nomad, liu2025citywalker], StereoWalker also requires gpus for robots deployment, and it is not directly runnable on the Clearpath Jackal onboard CPU. We therefore employ a FastAPI-based interface to connect the robot with a remote GPU server for real-time inference, as we do not have NVIDIA Jetson hardware, which is used in previous works.\nDespite this current limitation, the computational footprint of StereoWalker remains within the practical range of modern robotic platforms. Specifically, our model requires 2.89 GB of VRAM and 0.2 s per sample on an A100 GPU at inference time, compared to CityWalker which consumes 1.68 GB and 0.06 s per sample. This overhead remains manageable in our setting, as the model predicts five future waypoints spanning a five-second horizon. This allows the system to operate at a one-second inference interval, ensuring that new waypoint commands can be generated and executed between consecutive positions without causing stalls or instability in robot motion. As a result, even when accounting for network latency introduced by the FastAPI server, StereoWalker remains capable of real-robots deployment and achieve the best performance with minimal computation overhead compare to Citywalker, as evident in Fig. 4."
}]