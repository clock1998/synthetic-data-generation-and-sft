{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What is the name of the full-spectrum benchmark introduced in the article for evaluating the fidelity of generated world models?<|assistant|>WorldLens"}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What are the five aspects that WorldLens evaluates in a generated world model?<|assistant|>Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference"}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What is the purpose of WorldLens-26K, as mentioned in the article?<|assistant|>WorldLens-26K is a large-scale dataset of human-annotated videos with numerical scores and textual rationales, constructed to align objective metrics with human judgment"}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What is the role of WorldLens-Agent in the evaluation process, according to the article?<|assistant|>WorldLens-Agent is an evaluation model distilled from the annotations in WorldLens-26K, enabling scalable, explainable scoring"}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What does the article suggest is missing in the current evaluation methods for generated world models?<|assistant|>The article suggests that the field lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control"}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What is the title of the proposed framework in this work?<|assistant|>The title of the proposed framework is SceneMaker."}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What challenges does the SceneMaker framework address in open-set 3D scene generation?<|assistant|>The SceneMaker framework addresses the challenges of producing high-quality geometry and accurate poses under severe occlusion and open-set settings."}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>How does the SceneMaker framework decouple the de-occlusion model from 3D object generation?<|assistant|>The SceneMaker framework decouples the de-occlusion model from 3D object generation by enhancing the de-occlusion model with image datasets and collected de-occlusion datasets for more diverse open-set occlusion patterns."}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What is the purpose of the unified pose estimation model proposed in the SceneMaker framework?<|assistant|>The purpose of the unified pose estimation model proposed in the SceneMaker framework is to improve accuracy by integrating global and local mechanisms for both self-attention and cross-attention."}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What is the primary application of open-set 3D scene generation as mentioned in the article?<|assistant|>The primary application of open-set 3D scene generation, as mentioned in the article, is in AIGC and embodied AI, including applications such as 3D asset creation, simulation environment construction, and 3D perception for decision-making."}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What is the focus of the research presented in this paper?<|assistant|>The research presented in this paper focuses on empowering dynamic urban navigation by augmenting navigation foundation models (NFMs) with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking."}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>Why is the use of monocular vision alone inadequate for robust navigation according to the paper?<|assistant|>The paper argues that relying on monocular vision alone is inefficient because it ignores mid-level vision priors, requires large amounts of pixel-to-action supervision, and is especially challenging in dynamic and unstructured settings due to depth-scale ambiguity in monocular views."}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What advantages does the use of stereo vision offer over monocular input according to the paper?<|assistant|>The paper suggests that stereo vision yields higher navigation performance than monocular input because stereo inputs resolve the depth-scale ambiguity."}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What is the significance of the dataset curated in this research?<|assistant|>The dataset curated in this research is significant because it is a large stereo navigation dataset with automatic action annotation from Internet stereo videos, which supports the training of StereoWalker and facilitates future research."}
{"text": "<|system|>You are a helpful academic Q&A assistant specialized in scholarly content.<|user|>What is the conclusion of the experiments conducted in this research?<|assistant|>The conclusion of the experiments is that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data."}
